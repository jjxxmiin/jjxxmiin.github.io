---
layout: post
title: "BiRefNet: 고해상도 이미지 세분화를 위한 최첨단 AI 모델"
summary: "BiRefNet, 어떻게 미세한 이미지 분할을 정밀하게 수행할까? 최신 연구와 실용적 응용 사례를 소개합니다."
date: 2025-02-25
categories: paper
math: true
---

## **BiRefNet: 고해상도 이미지 세분화를 위한 최첨단 AI 모델**

- 📖 논문: [https://arxiv.org/abs/2401.03407v6](https://arxiv.org/abs/2401.03407v6)  
- 🖥️ 깃허브: [https://github.com/ZhengPeng7/BiRefNet](https://github.com/ZhengPeng7/BiRefNet)  
- 🤖 데모 실행: [https://fal.ai/models/fal-ai/birefnet/v2](https://fal.ai/models/fal-ai/birefnet/v2)  

> **🔍 연구 기관:** Nankai University, Aalto University, Shanghai AI Laboratory, University of Trento 등  
> **✍️ 저자:** Peng Zheng, Dehong Gao, Deng-Ping Fan, Li Liu, Jorma Laaksonen, Wanli Ouyang, Nicu Sebe  
> **📅 논문 발표:** 2024년 7월 24일  



![1](/assets/img/post_img/birefnet/1.PNG)



---

## 🎯 **BiRefNet이란?**
BiRefNet은 **고해상도 이진 이미지 분할(DIS, Dichotomous Image Segmentation)**을 위한 최신 AI 모델입니다.  
기존 이미지 분할 모델들은 고해상도 이미지에서 미세한 객체의 경계를 정확하게 처리하는 데 한계를 보였지만,  
BiRefNet은 **Bilateral Reference (양방향 참조) 전략**을 도입하여 **정밀한 세분화**를 수행합니다.  



![2](/assets/img/post_img/birefnet/2.PNG)



🔹 **BiRefNet의 특징**  
✔️ **고해상도 이미지 유지** → 축소 없이 원본 해상도로 분석 가능  
✔️ **Bilateral Reference (양방향 참조)** → 원본 이미지 + 경계 감지를 결합하여 정밀도 향상  
✔️ **Transformer 기반 특성 추출** → 더 넓은 문맥을 고려한 고급 분할 가능  
✔️ **다단계 학습 전략** → 빠르고 안정적인 학습 및 향상된 성능  

---

## 🔥 **BiRefNet의 핵심 기술**



![3](/assets/img/post_img/birefnet/3.PNG)




### **1️⃣ Bilateral Reference (양방향 참조) 아키텍처**

BiRefNet의 가장 혁신적인 부분은 **Bilateral Reference** 구조입니다. 이 구조는 두 가지 주요 경로로 구성됩니다:



![4](/assets/img/post_img/birefnet/4.PNG)



#### **🔍 Inward Reference(내부 참조) 모듈 상세 분석**

**내부 참조 모듈**은 원본 이미지의 고해상도 정보를 보존하는 역할을 합니다:

```python
def inward_reference_module(input_img, feature_map):
    # 원본 이미지에서 특징 추출
    original_features = extract_features(input_img)
    
    # 특징맵과 원본 특징의 어텐션 계산
    attention_weights = compute_attention(feature_map, original_features)
    
    # 어텐션 가중치를 적용하여 강화된 특징맵 생성
    enhanced_features = apply_attention(feature_map, attention_weights)
    
    return enhanced_features
```

이 모듈의 핵심은 원본 이미지의 정보를 네트워크 전체에 걸쳐 유지하는 것입니다. 특히, **Attention 메커니즘**을 사용하여 중요한 시각적 정보에 가중치를 부여합니다.

#### **🔎 Outward Reference(외부 참조) 모듈 상세 분석**

**외부 참조 모듈**은 객체의 경계를 정확하게 감지하는 데 특화되어 있습니다:

```python
def outward_reference_module(input_img, feature_map):
    # 경계 감지 연산 (Canny, Sobel 등 활용)
    edge_map = detect_edges(input_img)
    
    # 경계 특징 추출
    edge_features = process_edges(edge_map)
    
    # 특징맵과 경계 특징 결합
    combined_features = combine_features(feature_map, edge_features)
    
    return combined_features
```

이 모듈은 **다중 스케일 경계 감지(Multi-scale Edge Detection)**를 수행하여 다양한 크기의 객체 경계를 포착합니다. 특히 **경계 강화 메커니즘(Edge Enhancement Mechanism)**을 통해 미세한 경계도 놓치지 않도록 설계되었습니다.

### **2️⃣ Vision Transformer 기반 특징 추출 상세 분석**

BiRefNet은 **Vision Transformer(ViT)** 구조를 기반으로 하는 특징 추출기를 사용합니다:

```python
def vision_transformer_block(x, heads=8, dim_head=64, mlp_dim=2048):
    # 다중 헤드 셀프 어텐션
    attn_output = multi_head_self_attention(x, heads, dim_head)
    x = x + attn_output  # 잔차 연결
    
    # 정규화
    x = layer_norm(x)
    
    # MLP 블록
    mlp_output = mlp_block(x, mlp_dim)
    x = x + mlp_output  # 잔차 연결
    
    # 최종 정규화
    x = layer_norm(x)
    
    return x
```

BiRefNet의 Transformer 기반 특징 추출기는 다음과 같은 개선사항을 포함합니다:

- **Hierarchical Attention**: 다양한 수준의 세부 정보를 처리하기 위한 계층적 어텐션 메커니즘
- **Position-aware Encoding**: 이미지 내 객체의 위치 정보를 보존하는 위치 인코딩
- **Adaptive Token Fusion**: 토큰 정보를 적응적으로 결합하여 더 풍부한 특징 표현

### **3️⃣ 다단계 학습 전략 (Multi-Stage Supervision) 심층 분석**

BiRefNet의 학습 전략은 일반적인 end-to-end 학습과 달리, **다단계 감독(multi-stage supervision)** 방식을 채택하고 있습니다:

```python
def multi_stage_loss(predictions, ground_truth):
    # 픽셀 레벨 손실 (Pixel-level loss)
    pixel_loss = binary_cross_entropy(predictions['pixel'], ground_truth)
    
    # 경계 인식 손실 (Boundary-aware loss)
    boundary_loss = boundary_iou_loss(predictions['boundary'], ground_truth)
    
    # 구조적 유사도 손실 (Structural similarity loss)
    structure_loss = structural_similarity_loss(predictions['structure'], ground_truth)
    
    # 가중치 합산
    total_loss = 0.5 * pixel_loss + 0.3 * boundary_loss + 0.2 * structure_loss
    
    return total_loss
```

이 다단계 학습 방식은 다음과 같은 단계로 구성됩니다:

1. **단계별 가중치 초기화(Stagewise Weight Initialization)**: 각 학습 단계마다 최적의 초기 가중치를 설정
2. **점진적 난이도 증가(Progressive Difficulty)**: 쉬운 패턴부터 학습하고 점차 어려운 패턴으로 확장
3. **다중 목표 최적화(Multi-objective Optimization)**: 여러 손실 함수를 동시에 최적화

이 학습 전략은 **수렴 속도를 약 35% 향상**시키고, **과적합 위험을 25% 감소**시키는 효과가 있습니다.

---

## 🔍 **기존 모델과 BiRefNet의 차이점**  



| 주요 기능 | 기존 모델 (IS-Net, UDUN) | BiRefNet | 성능 차이 |
|-----------|----------------|----------------|----------------|
| 해상도 유지 | 축소 후 분석 | 원본 해상도 유지 | 미세 구조 보존율 62% → 94% |
| 객체 경계 감지 | 부분 손실 발생 | Bilateral Reference 활용 | 경계 정확도 78% → 91% |
| 학습 방식 | 단일 손실 기반 | 다단계 학습 적용 | 수렴 속도 35% 향상 |
| 성능 (DIS5K 벤치마크) | 평균 83~87% Sm | 최대 92.5% Sm | 5.6~9.5% 성능 향상 |
| 메모리 사용량 | 기준 | 12% 증가 | 성능 대비 합리적 증가 |
| 추론 시간 | 기준 | 15% 증가 | TensorRT 최적화 시 5% 감소 |



📌 **BiRefNet은 기존 모델 대비 미세한 객체의 경계를 더 정확하게 인식**하며,  
📌 **픽셀 단위 오류를 최소화하여 이미지 분할의 품질을 극대화**합니다.  

---

## 📊 **벤치마크 성능 평가**  

BiRefNet은 **DIS5K, HRSOD, COD 등의 데이터셋**에서 성능을 평가했습니다.  



![5](/assets/img/post_img/birefnet/5.PNG)






![6](/assets/img/post_img/birefnet/6.PNG)




### 🔹 **DIS5K 데이터셋 성능 비교** (고해상도 이진 이미지 분할)  

DIS5K 데이터셋은 다양한 해상도(최대 4K)의 5,470개 이미지로 구성되어 있으며, 복잡한 배경과 다양한 객체를 포함합니다.



| 모델 | Sm(↑) | Em(↑) | Fm(↑) | MAE(↓) | 메모리 사용량 | 처리 속도 |
|------|-------|-------|-------|-------|------------|----------|
| FCBFormer | 0.857 | 0.921 | 0.864 | 0.028 | 3.6GB | 32fps |
| IS-Net | 0.874 | 0.942 | 0.895 | 0.022 | 4.2GB | 28fps |
| UDUN | 0.886 | 0.948 | 0.902 | 0.019 | 5.8GB | 24fps |
| **BiRefNet** | **0.945** | **0.965** | **0.947** | **0.012** | 6.5GB | 21fps |



여기서:
- **Sm**: 구조적 유사도 측정(Structural Measure) - 객체의 전체적인 구조 유사성 측정
- **Em**: 개선된 정밀도 측정(Enhanced-alignment Measure) - 경계 정렬의 품질 측정
- **Fm**: F-measure - 정밀도와 재현율의 조화 평균
- **MAE**: 평균 절대 오차(Mean Absolute Error) - 낮을수록 좋음

BiRefNet은 UDUN 대비 **Sm 6.8%** 향상을 보이며, 특히 **복잡한 텍스처와 미세한 구조**가 있는 이미지에서 우수한 성능을 보입니다.

### 🔹 **HRSOD 성능 비교** (고해상도 객체 검출)  

HRSOD(High-Resolution Salient Object Detection) 데이터셋은 **2K 이상의 고해상도** 이미지로 구성됩니다.



| 모델 | maxF(↑) | Sm(↑) | Em(↑) | MAE(↓) | 추론 시간 |
|------|---------|-------|-------|-------|---------|
| SCRN | 0.818 | 0.837 | 0.901 | 0.045 | 0.08s |
| HDFNet | 0.831 | 0.855 | 0.920 | 0.039 | 0.12s |
| LDF | 0.850 | 0.872 | 0.934 | 0.031 | 0.10s |
| **BiRefNet** | **0.879** | **0.894** | **0.957** | **0.024** | 0.14s |



BiRefNet은 HRSOD에서 기존 SOTA 대비 **2.0% Sm 향상**을 달성했으며, 특히 **작은 객체와 복잡한 배경**에서의 분할 성능이 뛰어납니다.

### 🔹 **COD 성능 비교** (위장 객체 탐지)  

COD(Camouflaged Object Detection) 데이터셋은 **배경과 유사한 색상과 텍스처**를 가진 객체를 포함하는 어려운 이미지를 포함합니다.



| 모델 | Sm(↑) | Em(↑) | Fm(↑) | MAE(↓) | 하드웨어 요구사항 |
|------|-------|-------|-------|-------|---------------|
| PFNet | 0.782 | 0.841 | 0.775 | 0.085 | RTX 2080Ti |
| FSPNet | 0.793 | 0.854 | 0.801 | 0.073 | RTX 3090 |
| BGNet | 0.809 | 0.862 | 0.815 | 0.062 | RTX 3090 |
| **BiRefNet** | **0.849** | **0.912** | **0.857** | **0.038** | RTX 3090 |



BiRefNet은 FSPNet 대비 **5.6% 성능 향상**을 보이며, 특히 **배경과 유사한 객체**도 더 정확하게 분할합니다.

---

## 🖥️ 실제 사용 사례 (Use Cases)



![7](/assets/img/post_img/birefnet/7.PNG)



### 1️⃣ 의료 영상 분석 (Medical Image Segmentation)
✅ MRI, CT 스캔 이미지에서 미세한 병변을 정확하게 분할  
✅ 기존 모델보다 작은 크기의 암 조직도 감지 가능  

### 2️⃣ 자율주행 차량의 객체 인식
✅ 고해상도 카메라 영상을 분석하여 차선, 보행자, 차량을 더 정확하게 감지  
✅ 야간, 안개 환경에서도 높은 정확도 유지  

### 3️⃣ 위성 이미지 분석 (Satellite Image Processing)
✅ 도시 개발, 삼림 보호 등의 환경 변화 감지  
✅ 초고해상도 위성 사진에서도 정밀한 객체 분류 가능  

---

## 🔮 향후 발전 방향
### 1️⃣ 연산 속도 최적화
현재 모델은 고해상도 처리를 위해 연산량이 크므로 경량화된 모델 버전을 연구 중
### 2️⃣ 실시간 세분화 모델 개발
비디오 스트림에서도 실시간으로 세분화가 가능하도록 추가적인 연구 진행 예정
### 3️⃣ 3D 이미지 분할로 확장
현재 2D 이미지 기반 모델에서 3D 의료 영상, LiDAR 데이터 분석까지 적용할 수 있도록 개선

---

## 🎯 결론
BiRefNet은 고해상도 이미지에서 미세한 객체 분할을 가능하게 하는 강력한 AI 모델입니다.

✅ Bilateral Reference 기법으로 기존 모델 대비 6~8% 성능 향상  
✅ 고해상도 원본 이미지를 유지하면서 더 세밀한 분석 가능  
✅ 자율주행, 의료 영상, 위성 이미지 분석 등 다양한 응용 가능  

📢 미래의 AI 기반 이미지 세분화는 더욱 정교해질 것입니다. BiRefNet이 그 중심에 있습니다! 🚀

---

## 🛠️ 설치 및 사용법  

BiRefNet을 실행하려면 **Python 환경 설정**, **모델 가중치 다운로드**, 그리고 **추론 실행**이 필요합니다.   

다음 단계를 차례대로 수행하세요.

### **📌 1. 환경 설정**  

먼저 Python 환경을 설정하고 필요한 패키지를 설치합니다.

```bash
# BiRefNet 실행을 위한 가상 환경 생성
conda create -n birefnet python=3.10 -y
conda activate birefnet

# 필수 패키지 설치
pip install -r requirements.txt
```

BiRefNet은 PyTorch 2.5.1 + CUDA 12.4 또는 PyTorch 2.0.1 + CUDA 11.8 환경에서 최적화됩니다. 가능하면 해당 버전을 사용하세요.

### 📌 2. 모델 가중치 다운로드

BiRefNet을 실행하려면 사전 학습된 가중치(weight files) 를 다운로드해야 합니다.

```bash
# weights 폴더 생성
mkdir weights

# 모델 가중치 다운로드 (최신 버전 사용 권장)
wget -P weights https://github.com/ZhengPeng7/BiRefNet/releases/download/v1.0/BiRefNet_weights.pth
```

또는, 공식 Google Drive 링크에서 직접 다운로드할 수도 있습니다.

### 📌 3. 데이터셋 다운로드

BiRefNet을 학습하거나 테스트하려면 **DIS, COD, HRSOD 등의 데이터셋**이 필요합니다.

```bash
# 공식 제공 데이터셋 다운로드
wget -P datasets https://example.com/dataset/DIS.zip
unzip datasets/DIS.zip -d datasets/

wget -P datasets https://example.com/dataset/COD.zip
unzip datasets/COD.zip -d datasets/
```

(※ 실제 데이터셋 URL은 공식 GitHub 또는 논문에서 확인 필요)

### 📌 4. 추론(Inference) 실행

BiRefNet을 이용하여 이미지를 입력하고 결과를 확인하는 방법입니다.

```bash
# 단일 이미지 분할 실행
python inference.py --image_path sample.jpg --output_path result.jpg
```

`sample.jpg` → 분할할 입력 이미지  
`result.jpg` → 출력된 세그멘테이션 결과 이미지  

만약 여러 개의 이미지를 한 번에 처리하고 싶다면:

```bash
# 다중 이미지 추론 실행
python batch_inference.py --input_dir images/ --output_dir results/
```

### 📌 5. Colab을 활용한 실행 (간편 실행)
BiRefNet은 Google Colab에서 실행할 수도 있습니다.
다음 Colab 링크를 열고 실행하면 바로 추론 결과를 확인할 수 있습니다.

- Google Colab: [https://colab.research.google.com/drive/1MaEiBfJ4xIaZZn0DqKrhydHB8X97hNXl](https://colab.research.google.com/drive/1MaEiBfJ4xIaZZn0DqKrhydHB8X97hNXl)
- Google Colab with Box Guided: [https://colab.research.google.com/drive/1B6aKZ3ekcvKMkSBn0N5mCASLUYMp0whK](https://colab.research.google.com/drive/1B6aKZ3ekcvKMkSBn0N5mCASLUYMp0whK)

### 📌 6. ONNX 변환 및 TensorRT 배포 (고속 추론)

BiRefNet을 ONNX 포맷으로 변환하여 배포할 수도 있습니다.

```bash
# PyTorch 모델을 ONNX로 변환
python export_onnx.py --weights weights/BiRefNet_weights.pth --output weights/BiRefNet.onnx

# TensorRT 변환 (추론 속도 최적화)
python export_tensorrt.py --onnx weights/BiRefNet.onnx --output weights/BiRefNet.trt
```

TensorRT를 사용하면 RTX 4090 기준, 17FPS 이상 속도 향상이 가능합니다.