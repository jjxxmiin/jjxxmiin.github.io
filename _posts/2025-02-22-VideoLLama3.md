---
layout: post
title: "VideoLLaMA3 í›‘ì–´ë³´ê¸°"
summary: "xAIê°€ ê°œë°œí•œ ìµœì‹  LLM, Grok 3ì˜ ê¸°ìˆ ì  í˜ì‹ ê³¼ ì„±ëŠ¥ ë¶„ì„"
date: 2025-02-21 16:00 -0400
categories: AI
math: true
---

## ğŸš€ VideoLLaMA 3: ìµœì²¨ë‹¨ ë©€í‹°ëª¨ë‹¬ ë¹„ë””ì˜¤ ì´í•´ ëª¨ë¸  

### ğŸ” ê°œìš”  
> ğŸ“„ **ë…¼ë¬¸:** [https://arxiv.org/abs/2501.13106](https://arxiv.org/abs/2501.13106)  
> ğŸ› ï¸ **GitHub:** [https://github.com/DAMO-NLP-SG/VideoLLaMA3](https://github.com/DAMO-NLP-SG/VideoLLaMA3)  

**VideoLLaMA 3**ëŠ” ì´ë¯¸ì§€ ë° ë¹„ë””ì˜¤ ì´í•´ë¥¼ ìœ„í•œ **ìµœì‹  ë©€í‹°ëª¨ë‹¬ ê¸°ë°˜ ëª¨ë¸**ë¡œ,  
ì‹œê°„ì  íŠ¹ì„±ì„ ë°˜ì˜í•œ **ë¹„ì „ ì¤‘ì‹¬(vision-centric) í•™ìŠµ íŒ¨ëŸ¬ë‹¤ì„**ê³¼ **í”„ë ˆì„ì›Œí¬ ë””ìì¸**ì„ ì ìš©í•˜ì—¬ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.  



![1](/assets/img/post_img/videollama3/1.PNG)



---

## ğŸ¯ ì£¼ìš” íŠ¹ì§•  



![2](/assets/img/post_img/videollama3/2.PNG)



### ğŸ”¥ ë¹„ì „ ì¤‘ì‹¬(vision-centric) í•™ìŠµ íŒ¨ëŸ¬ë‹¤ì„  
ê¸°ì¡´ì˜ ë¹„ë””ì˜¤-í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ì€ í’ˆì§ˆì´ ë‚®ê±°ë‚˜ ë¶€ì¡±í•œ ê²½ìš°ê°€ ë§ìŒ.  
ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ **ê³ í’ˆì§ˆ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ë°ì´í„°** ì¤‘ì‹¬ì˜ í•™ìŠµì„ ì ìš©í•¨.  

ğŸ“Œ **4ë‹¨ê³„ í•™ìŠµ ê³¼ì •**  
1ï¸âƒ£ **ë¹„ì „ ì¸ì½”ë” ì ì‘ (Vision Encoder Adaptation)**  
2ï¸âƒ£ **ë¹„ì „-ì–¸ì–´ ì •ë ¬ (Vision-Language Alignment)**  
3ï¸âƒ£ **ë©€í‹°íƒœìŠ¤í¬ íŒŒì¸íŠœë‹ (Multi-task Fine-tuning)**  
4ï¸âƒ£ **ë¹„ë””ì˜¤ ì¤‘ì‹¬ íŒŒì¸íŠœë‹ (Video-centric Fine-tuning)**  

---

## ğŸ¬ í˜ì‹ ì ì¸ ë¹„ë””ì˜¤ ì²˜ë¦¬ ê¸°ìˆ   



![3](/assets/img/post_img/videollama3/3.PNG)



### 1ï¸âƒ£ Any-resolution Vision Tokenization (AVT)  
âœ” ë‹¤ì–‘í•œ í•´ìƒë„ ì²˜ë¦¬ ê°€ëŠ¥  
âœ” ë¹„ë””ì˜¤ ë°ì´í„°ì˜ **ê³ í•´ìƒë„ ì •ë³´ ë³´ì¡´**  



![4](/assets/img/post_img/videollama3/4.PNG)



### 2ï¸âƒ£ Differential Frame Pruner (DiffFP)  
âœ” ì¤‘ë³µ í”„ë ˆì„ì„ ì œê±°í•˜ì—¬ **ì—°ì‚°ëŸ‰ ê°ì†Œ**  
âœ” ì¤‘ìš”í•œ ì •ë³´ë§Œ ìœ ì§€í•˜ì—¬ **íš¨ìœ¨ì ì¸ ë¹„ë””ì˜¤ ì²˜ë¦¬**  

### 3ï¸âƒ£ ê³ í’ˆì§ˆ ë°ì´í„°ì…‹ í™œìš©
- **VL3-Syn7M** ë°ì´í„°ì…‹ êµ¬ì¶• (7ë°±ë§Œ ê°œì˜ ê³ í’ˆì§ˆ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìŒ)  
- **OCR ë°ì´í„°, ì°¨íŠ¸ ë¶„ì„ ë°ì´í„°, ìˆ˜í•™ì  ì‹œê°ì  ë¬¸ì œ í•´ê²° ë°ì´í„° í¬í•¨**  

### 4ï¸âƒ£ ëŒ€ê·œëª¨ ì‚¬ì „ í•™ìŠµ
- OpenAI, Meta ë“±ì˜ ìµœì‹  ì—°êµ¬ ë°˜ì˜í•œ **Qwen2.5 LLM** ëª¨ë¸ ê¸°ë°˜  
- ì‚¬ì „ í›ˆë ¨ëœ **SigLIP ë¹„ì „ ì¸ì½”ë”** ê°œì„   



![5](/assets/img/post_img/videollama3/5.PNG)



---

## ğŸ“Š ì„±ëŠ¥ í‰ê°€  

### ğŸ–¼ï¸ ì´ë¯¸ì§€ ì´í•´ ì„±ëŠ¥  



![9](/assets/img/post_img/videollama3/9.PNG)






![10](/assets/img/post_img/videollama3/10.PNG)






| ëª¨ë¸ | ChartQA | DocVQA | MathVista | MMMU-Pro | RealWorldQA |
|---|---|---|---|---|---|
| **VideoLLaMA 3 (7B)** | **86.3** | **94.9** | **67.1** | **33.6** | **72.7** |
| Qwen2-VL 7B | 83.0 | 94.5 | 58.2 | 31.4 | 70.1 |
| LLaVA-OneVision | 80.0 | 87.5 | 63.2 | 24.1 | 66.3 |



### ğŸ¬ ë¹„ë””ì˜¤ ì´í•´ ì„±ëŠ¥  



![9](/assets/img/post_img/videollama3/11.PNG)






![10](/assets/img/post_img/videollama3/12.PNG)






| ëª¨ë¸ | VideoMME | PerceptionTest | MLVU | TempCompass | NextQA |
|---|---|---|---|---|---|
| **VideoLLaMA 3 (7B)** | **66.2** | **72.8** | **73.0** | **68.1** | **84.5** |
| InternVL2.5 8B | 64.2 | 68.9 | 69.0 | 68.3 | 85.0 |
| Qwen2-VL 7B | 63.3 | 62.3 | 69.8 | 67.9 | 81.2 |



âœ… **ëŒ€ë¶€ë¶„ì˜ ë²¤ì¹˜ë§ˆí¬ì—ì„œ SOTA ì„±ëŠ¥ ë‹¬ì„±!**  

---

## ğŸ› ï¸ ì„¤ì¹˜ ë° ì‚¬ìš©ë²•  

### ğŸ“Œ ê¸°ë³¸ í™˜ê²½ ì„¤ì •  

```bash
pip install torch==2.4.0 torchvision==0.17.0 --extra-index-url https://download.pytorch.org/whl/cu118
pip install flash-attn --no-build-isolation
pip install transformers==4.46.3 accelerate==1.0.1
pip install decord ffmpeg-python imageio opencv-python
```

### ğŸ“Œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

```bash
git clone https://github.com/DAMO-NLP-SG/VideoLLaMA3
cd VideoLLaMA3
pip install -r requirements.txt
```

### ğŸ“Œ ì¶”ë¡ (Inference) ì½”ë“œ ì˜ˆì œ

```python
import torch
from transformers import AutoModelForCausalLM, AutoProcessor

device = "cuda:0"
model_path = "DAMO-NLP-SG/VideoLLaMA3-7B"
model = AutoModelForCausalLM.from_pretrained(
    model_path, trust_remote_code=True, device_map={"": device},
    torch_dtype=torch.bfloat16, attn_implementation="flash_attention_2",
)
processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)

conversation = [
    {"role": "system", "content": "You are a helpful assistant."},
    {
        "role": "user",
        "content": [
            {"type": "video", "video": {"video_path": "./assets/cat_and_chicken.mp4", "fps": 1, "max_frames": 180}},
            {"type": "text", "text": "What is the cat doing?"}
        ]
    },
]

inputs = processor(conversation=conversation, add_system_prompt=True, add_generation_prompt=True, return_tensors="pt")
inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}
output_ids = model.generate(**inputs, max_new_tokens=1024)
response = processor.batch_decode(output_ids, skip_special_tokens=True)[0].strip()
print(response)
```

---

## ğŸ” í™œìš© ì‚¬ë¡€

### ğŸ–¼ï¸ ì°¨íŠ¸ ë¶„ì„ (Chart Understanding)



![6](/assets/img/post_img/videollama3/6.PNG)



ğŸ“Œ ì§ˆë¬¸: ì´ ì£¼ì‹ì€ ë³´ìœ í•  ê°€ì¹˜ê°€ ìˆì„ê¹Œ?  
ğŸ“Œ VideoLLaMA 3ì˜ ë‹µë³€: "í•´ë‹¹ ì£¼ì‹ì€ ë³€ë™ì„±ì´ í¬ê³  íˆ¬ì ìœ„í—˜ì´ ë†’ì•„ ë³´ì…ë‹ˆë‹¤."

### ğŸ“„ OCR ë° ë¬¸ì„œ ì´í•´ (Document Understanding)



![7](/assets/img/post_img/videollama3/7.PNG)



ğŸ“Œ ì§ˆë¬¸: ë¬¸ì„œì˜ ë‚´ìš©ì„ ìš”ì•½í•´ ì£¼ì„¸ìš”.  
ğŸ“Œ VideoLLaMA 3ì˜ ë‹µë³€: "ë¬¸ì„œì—ì„œ ì½ì€ ì£¼ìš” ë‚´ìš©ì€..."

### ğŸ¬ ë¹„ë””ì˜¤ ìº¡ì…˜ ìƒì„± (Video Captioning)



![8](/assets/img/post_img/videollama3/8.PNG)



ğŸ“Œ ì§ˆë¬¸: ì´ ë¹„ë””ì˜¤ì˜ ë‚´ìš©ì„ ì„¤ëª…í•´ ì£¼ì„¸ìš”.  
ğŸ“Œ VideoLLaMA 3ì˜ ë‹µë³€: "ì´ ë¹„ë””ì˜¤ëŠ” ìš°ì£¼ì„ ì´ ê¶¤ë„ë¥¼ ë„ëŠ” ì¥ë©´ìœ¼ë¡œ ì‹œì‘ë©ë‹ˆë‹¤..."

---

## ğŸš€ ê²°ë¡ 
VideoLLaMA 3ëŠ” ìµœì‹  ë©€í‹°ëª¨ë‹¬ AI ëª¨ë¸ ì¤‘ ìµœê°•ì˜ ì„±ëŠ¥ì„ ì œê³µí•˜ë©°,
íŠ¹íˆ ë¹„ë””ì˜¤ ë° ì´ë¯¸ì§€ ì´í•´ì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë°œíœ˜í•©ë‹ˆë‹¤.

âœ” ë¹„ì „ ì¤‘ì‹¬ í•™ìŠµ íŒ¨ëŸ¬ë‹¤ì„ ì ìš©  
âœ” SOTA ì„±ëŠ¥ ë‹¬ì„± (ìµœì‹  ë²¤ì¹˜ë§ˆí¬ 1ìœ„ ê¸°ë¡)  
âœ” ë¹„ë””ì˜¤ ìº¡ì…˜, OCR, ì°¨íŠ¸ ë¶„ì„, ë¬¸ì„œ ì´í•´ ë“± ë‹¤ì–‘í•œ í™œìš© ê°€ëŠ¥  