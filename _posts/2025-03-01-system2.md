---
layout: post
title: "인간처럼 '생각'하는 AI: Reasoning LLMs의 발전 방향"
summary: "논리적 추론이 가능한 LLM이 왜 중요한지, 어떻게 발전할 수 있는지 쉽게 설명합니다."
date: 2025-02-28
categories: paper
math: true
---

## 🚀 왜 '생각하는 AI'가 필요한가?

> **논문 요약:** [“From System 1 to System 2: A Survey of Reasoning Large Language Models”](https://arxiv.org/abs/2502.17419v2)  
> **저자:** Microsoft Research, OpenAI, Google DeepMind  
> **발표:** 2025년 2월  

최근 인기 있는 ChatGPT, Claude, Gemini 같은 대형 언어 모델(LLM)은 뛰어난 언어 능력을 보여줍니다. 하지만 이들 모델은 주로 ‘직관적 패턴 매칭’에 의존하여 텍스트를 생성한다는 점에서 아직 **복잡한 논리적 사고**나 **다단계 추론** 능력은 부족하다고 평가됩니다.

이 논문은 사람들이 생각하듯이 논리적으로 추론(‘System 2 사고’)하는 AI 모델을 만들기 위해 어떤 기술이 필요한지를 정리하고, 앞으로 어떤 발전이 기대되는지 안내합니다.



![1](/assets/img/post_img/system2/1.jpg)



---

## 🧩 System 1 vs. System 2: 직관과 논리

우리가 생각하는 방식은 크게 두 가지로 나누어 설명할 수 있습니다. 심리학자 Daniel Kahneman은 이를 **System 1**과 **System 2**로 구분했습니다.

| 사고 방식   | 특징                                           | 인간의 예시                  | AI 모델과의 유사점                      |
|-------------|------------------------------------------------|------------------------------|-----------------------------------------|
| **System 1** | 빠르고 직관적이며 무의식적 사고                | 얼굴 인식, 간단한 계산(2+2=4) | 기존 GPT-4, Claude 등 (주로 패턴 매칭)   |
| **System 2** | 논리적·분석적이고 의식적으로 노력해야 하는 사고 | 복잡한 수학 풀이, 전략적 계획 | ‘Reasoning LLM’(추론 중심 LLM) 목표        |

- **System 1 (직관)**:  
  - 빠른 속도로 문제를 ‘직감’적으로 처리  
  - 하지만 복잡하거나 단계가 여러 개인 문제를 풀기에는 한계가 있음  

- **System 2 (논리)**:  
  - 시간을 들여 체계적으로 사고하고 분석  
  - 다단계 추론과 자기 교정이 가능  

현재 대부분의 LLM은 **System 1** 수준에 머물러 있어, 복잡한 논리 문제나 단계적 추론에는 취약합니다.  
다음 단계로 넘어가기 위해서는 **System 2** 수준의 사고방식을 어떻게 AI에 적용할지가 핵심 과제가 됩니다.

---

## 🔑 System 2 추론을 위한 다섯 가지 핵심 기술

논문에서는 **논리적 사고가 가능한 ‘Reasoning LLM’**을 만들기 위해 아래 다섯 가지 기술을 강조합니다.



![System 2 추론을 위한 핵심 기술 요소](/assets/img/post_img/system2/2.png)



### 1. 구조적 탐색 (Structured Search)
- 단순히 ‘다음 단어’를 예측하는 것이 아니라, 여러 경우의 수를 **탐색**해보는 방식  
- 예시:  
  - **체스 엔진**이 가능한 모든 수를 계산해가며 최적 해법을 찾는 것  
  - 복잡한 **수학 문제**를 풀 때 여러 풀이 과정을 시도해보는 것  

### 2. 보상 모델링 (Reward Modeling)
- **논리적으로 올바른 답을 우선시**하도록 AI에 보상 함수를 설계  
- RLHF(Reinforcement Learning from Human Feedback) 등을 통해 **“정확하고 논리적인 답변”**을 강화  
- 점수화 기준:  
  - 단계별 추론이 맞는지  
  - 최종 결론이 합리적인지  

### 3. 자기 개선 (Self-Improvement)
- **AI 스스로** 오류를 찾아내고 수정하는 능력  
- 주요 전략:  
  - **역추적(Backtracking)**: 잘못된 경로라고 판단되면 되돌아가 재추론  
  - **자기 비평(Self-criticism)**: 생성한 답변을 스스로 검토하고 잘못된 부분 수정  
  - **다중 경로 탐색(Multi-path exploration)**: 여러 아이디어를 동시에 시도한 뒤, 그중 최적의 답안 선택  

### 4. 매크로 액션 (Macro Actions)
- 복잡한 문제를 해결할 때, **단계적으로 사고**하도록 유도  
- 예시 기법:  
  - **Chain-of-Thought (CoT)**: “단계별로 생각해보자”와 같이 풀 과정을 구체적으로 쓰도록 유도  
  - **Tree-of-Thought (ToT)**: 여러 가지 추론 경로를 나무 구조로 탐색  

### 5. 강화 미세 조정 (Reinforcement Fine-Tuning, RFT)
- **추론 특화 데이터셋**으로 미세 조정하여 논리적 사고 능력을 높임  
- 접근법:  
  - **분야별 학습**(예: 수학·코딩·법률·의료)  
  - **난이도 점진 상승**: 쉬운 문제부터 시작해 점점 더 어려운 문제로 학습  
  - **전문가 시연 학습**: 인간 전문가의 추론 과정을 따라 하며 학습  

---

## 📊 최신 Reasoning LLM 성능: 어디까지 왔나?

아래는 논문에서 다룬 대표적인 **추론 벤치마크**와 모델 성능 예시입니다. (모든 수치는 예시)

| 분야           | 대표 벤치마크               | 평가 항목                  | 최신 모델 성능                         |
|----------------|-----------------------------|----------------------------|----------------------------------------|
| **수학**       | AIME 2024, Olympiad Bench   | 복잡한 문제풀이 능력       | GPT-4T: 42.3%, Claude 3.7: 47.8%        |
| **코딩**       | Codeforces, SWE-Bench       | 알고리즘 코딩 문제 해결     | Gemini 1.5 Pro: 61.2%, DeepSeek Coder: 68.7% |
| **과학**       | GPQA Diamond, MMLU-Pro      | 전문 지식 및 논리 추론      | Claude 3.7: 83.4%, GPT-4T: 79.6%         |
| **멀티모달**   | MMMU, MathVision            | 이미지·텍스트 혼합 추론      | Gemini 1.5 Pro: 55.7%, GPT-4o: 58.2%       |
| **에이전트**   | WebShop, SciWorld           | 목표 달성 및 상호작용 능력  | DeepMind Agent: 67.3%, Claude Agent: 62.1% |
| **의료**       | JAMA Clinical, MedQA        | 임상 사례 분석             | Med-PaLM 3: 88.9%, GPT-4T: 85.3%         |

대체로 **기본 질의응답(System 1)**에서는 좋은 성능을 보이지만, **논리적 사고(System 2)**를 요하는 문제에서는 사람만큼 뛰어나지는 못합니다. 특히 여러 단계를 거치는 복합적 추론이나, 답을 스스로 수정하는 능력은 아직 제한적입니다.

---

## 🔮 미래 방향: 진짜 ‘생각’하는 AI로



![향후 연구 방향](/assets/img/post_img/system2/3.PNG)



논문에서는 앞으로 **System 2**에 가까운 AI를 만들기 위해 다음 세 가지 분야에 집중해야 한다고 말합니다.

### 1. 자기 학습(Self-Learning) 강화
- **자기 교정(Self-Correction) 모델**을 더 발전시켜 AI가 직접 오류를 고칠 수 있도록  
- “**AI가 AI를 가르친다**”는 개념을 실제 적용  
- 예:  
  - Google DeepMind의 "Recursive AI Training"  
  - OpenAI의 "LLM Reflection"  
  - Microsoft의 "Self-Debug"  

### 2. 장기 추론(Long-Horizon Reasoning)
- 짧은 문맥 이상의 **장기 맥락**을 파악해가며 추론할 수 있는 능력  
- 유망한 접근법:  
  - **Memory-augmented 아키텍처**: 외부 메모리나 도구를 적극 활용  
  - **재귀적 추론(Recursive reasoning)**: 큰 문제를 잘게 나누어 단계적으로 풀어내기  
  - **메타인지(Metacognition)**: ‘내가 지금 무슨 생각을 하고 있는지’ 스스로 모니터링하고 수정  

### 3. 더욱 정교한 평가 시스템
- 기존 벤치마크로는 AI가 실제로 얼마나 ‘깊이 있는 사고’를 하는지 파악하기 어려움  
- 앞으로 필요할 평가 요소:  
  - **실시간 상호작용 테스트**: 단순 Q&A가 아닌 실제 환경에서 문제 해결 능력 평가  
  - **멀티모달 추론 평가**: 텍스트뿐만 아니라 이미지·음성·영상 등 다양한 입력을 활용한 복합 과제  
  - **인간 전문가와의 직접 비교**: 전문가 수준의 추론 능력을 갖췄는지 평가  

---

## 🎯 마무리: 패턴 인식에서 ‘진짜 추론’으로



![마무리](/assets/img/post_img/system2/4.PNG)



**System 1** 수준에 머물러 있는 현재의 LLM들은 이미 언어 처리와 정보 검색에서 훌륭한 성능을 보입니다.  
하지만 진정으로 **‘인간처럼 생각하는’ AI**가 되기 위해서는, **체계적이고 자기 교정이 가능한 System 2** 수준으로 도약해야 합니다.

- **단순 패턴 매칭** → **체계적 추론**  
- **빠른 응답** → **깊이 있는 문제 해결**

논문에서 제시한 다섯 가지 핵심 기술과 새로운 평가 체계를 통해, AI가 사람의 사고 방식을 더 닮아갈 수 있을 것입니다. 앞으로의 AI가 복잡한 과학 연구, 의학 진단, 법률 자문, 교육, 창작 등 다양한 영역에서 **“스스로 생각하고 수정”** 하는 모습으로 발전하길 기대해 봅시다.

> **기술의 목표는 단순히 인간을 흉내 내는 것이 아닌, 우리를 더욱 풍요롭게 해주는 ‘협력자(AI 파트너)’가 되는 것입니다.**
