---
layout: post
title: "DeepGEMM 훑어보기"
summary: "DeepGEMM: NVIDIA Hopper 아키텍처에서 FP8 General Matrix Multiplication(GEMM)을 극한까지 최적화한 라이브러리"
date: 2025-02-26
categories: paper
math: true
---

## **DeepGEMM: NVIDIA Hopper GPU를 위한 초고속 FP8 행렬 연산 라이브러리**



![1](/assets/img/post_img/deepseek/logo.png)



### 🔗 프로젝트 개요
- **📖 프로젝트:** *DeepGEMM: Clean and Efficient FP8 General Matrix Multiplication (GEMM)*
- **🏢 개발사:** *DeepSeek-AI*
- **🔗 GitHub:** [DeepGEMM Repository](https://github.com/deepseek-ai/DeepGEMM)
- **🎯 주요 특징:**
  - **FP8 연산 최적화** (Hopper Tensor Core 전용)
  - **Mix-of-Experts (MoE) 모델 지원**
  - **Just-In-Time (JIT) 컴파일 방식**으로 커널을 설치 없이 즉시 실행
  - **CUTLASS 및 CuTe 기반의 고성능 GEMM 커널** 제공



![1](/assets/img/post_img/deepseek/benchmark.png)



---

## 🎯 **DeepGEMM이란?**
인공지능 모델이 동작할 때 가장 많이 하는 작업이 무엇일까요? 바로 행렬 곱셈입니다. ChatGPT, Claude와 같은 대형 AI 모델들은 수십억 개의 행렬 곱셈을 수행하면서 텍스트를 이해하고 생성합니다.
DeepGEMM은 이러한 행렬 곱셈(GEMM)을 NVIDIA의 최신 GPU인 Hopper 아키텍처(H100, H800)에서 극도로 빠르게 처리할 수 있도록 만든 특별한 소프트웨어 라이브러리입니다.

### 💡 **일상생활의 비유로 이해하기**

- **기존 행렬 곱셈 = 일반 도로로 출퇴근하기**
- **DeepGEMM의 FP8 최적화 = 전용 고속도로로 출퇴근하기**

같은 거리를 가더라도 고속도로는 더 빠르고 효율적이죠? DeepGEMM은 AI 모델이 사용하는 '계산 고속도로'를 최적화한 것입니다.

---

### 🔍 **왜 FP8이 중요한가요?**

FP8은 '8비트 부동소수점'의 약자로, 숫자를 저장하는 방식입니다.

### 기존 방식과 FP8 비교:
- **FP32 (32비트)**: 매우 정밀하지만 메모리 사용량이 큼
- **FP16 (16비트)**: 중간 정도의 정밀도와 메모리 사용량
- **FP8 (8비트)**: 적은 메모리 사용량, 더 빠른 처리 속도

이것을 실생활에 비유하면:
- **FP32**: 고급 DSLR 카메라로 사진 찍기 (고화질, 큰 파일 크기)
- **FP16**: 스마트폰 카메라로 사진 찍기 (적당한 화질, 중간 파일 크기) 
- **FP8**: 압축된 이미지 (작은 파일 크기, 약간 화질 저하)

AI 모델에서는 대부분의 경우 FP8로도 충분한 정확도를 얻을 수 있어, 메모리 사용량과 처리 속도에서 큰 이득을 얻을 수 있습니다.

---

## 🔥 **DeepGEMM의 핵심 기술**

### **1️⃣ FP8 연산 최적화**
✅ FP8 Tensor Core 연산의 **부정확성을 보완하는 2단계 Accumulation 기법**  
✅ CUDA Core 기반의 **두 단계 연산(Promotion) 적용** → **FP8 연산의 정밀도 향상**  
✅ 기존 8-bit INT8 연산 대비 **더 빠르고 정확한 FP8 연산 제공**  

NVIDIA Hopper GPU에는 '텐서 코어'라는 특별한 하드웨어가 있는데, DeepGEMM은 이를 최대한 활용합니다. FP8 연산의 단점인 정밀도 문제를 해결하기 위해 **2단계 누적 기법**을 사용하여 정확도를 높였습니다.

### **2️⃣ Just-In-Time (JIT) 컴파일**
✅ **설치 시 별도 컴파일 필요 없음**  
✅ 실행할 때 최적의 커널을 **자동으로 JIT 컴파일하여 최상의 성능 제공**  
✅ **CUDA 12.8 이상에서 최고 성능**  

일반적인 라이브러리는 미리 모든 것을 컴파일해 두지만, DeepGEMM은 **실제로 필요한 시점에 최적의 코드를 생성**합니다. 마치 주문 요리처럼, 필요할 때 가장 신선하게 만들어 사용합니다.

### **3️⃣ Mix-of-Experts (MoE) 모델 지원**
✅ **Grouped GEMM 지원 (Contiguous & Masked Layout)**  
✅ 각 전문가(Expert) 블록을 최적화하여 **GPU 연산 효율 극대화**  

최근 AI 모델들은 여러 '전문가' 신경망을 함께 활용하는 MoE 구조를 많이 사용합니다. DeepGEMM은 이러한 구조에 최적화되어 있어 **MoE 기반 AI 모델의 성능을 크게 향상**시킵니다.

### **4️⃣ Tensor Memory Accelerator (TMA) 활용**
✅ **Hopper GPU의 TMA 기능을 활용하여 데이터 이동 최적화**  
✅ **LHS (좌변 행렬), RHS (우변 행렬) 및 Scaling Factor의 TMA Load/Store 지원**  
✅ GPU 메모리 대역폭을 최적화하여 **더 빠른 연산 속도 제공**  

GPU에서는 계산 자체보다 **데이터를 옮기는 과정이 더 많은 시간을 소비**하는 경우가 많습니다. DeepGEMM은 Hopper GPU의 TMA(Tensor Memory Accelerator) 기능을 사용해 데이터 이동을 최적화합니다.

### **5️⃣ FFMA (Fused Multiply-Add) SASS 최적화**
✅ FFMA 명령어를 **SASS(저수준 어셈블리)에서 직접 최적화**  
✅ CUDA Warp 간 교차 실행 (Interleaving) 적용  
✅ **FP8 연산에서 10% 이상의 성능 향상**  

자동차 엔진의 내부 부품까지 최적화하듯, DeepGEMM은 GPU의 가장 기본적인 연산 명령어인 FFMA(Fused Multiply-Add)를 저수준에서 직접 최적화했습니다.

---

## 📊 **얼마나 빨라졌나요?**

### **일반 AI 모델에서의 성능 향상**
DeepGEMM은 기존 기술(CUTLASS) 대비 **최대 2.7배 빠른 성능**을 보여줍니다.

| 행렬 크기 (M×N×K) | 성능 향상 |
|-----------------|---------|
| 64×2112×7168    | **2.7배** |
| 64×24576×1536   | **1.7배** |
| 128×7168×2048   | **1.7배** |

### **MoE 모델에서의 성능 향상**
여러 전문가 모델을 사용하는 MoE 구조에서도 약 **1.2배의 성능 향상**을 보여줍니다.

---

## 🚀 **실제 사용 시 이점**

1. **더 빠른 AI 모델 추론**: 같은 하드웨어로 AI 응답 시간 단축
2. **에너지 효율성 증가**: 같은 작업에 더 적은 전력 소비
3. **메모리 사용량 감소**: 더 큰 AI 모델을 같은 GPU에서 실행 가능
4. **비용 절감**: 적은 수의 GPU로 같은 성능 달성 가능

---

## 🛠️ **DeepGEMM 사용하기**

### **1️⃣ 요구 사항**
✅ **NVIDIA Hopper GPU (H100, H800)**  
✅ **CUDA 12.3 이상 (권장: 12.8 이상)**  
✅ **PyTorch 2.1 이상**  
✅ **CUTLASS 3.6 이상 (Git Submodule 포함)**  

### **2️⃣ 설치 방법**
```bash
git clone --recursive git@github.com:deepseek-ai/DeepGEMM.git
cd DeepGEMM
python setup.py install
```

### 3️⃣ 테스트 실행
```bash
python tests/test_jit.py  # JIT 컴파일 테스트
python tests/test_core.py  # GEMM 구현 테스트
```

## 🔮 미래 전망 및 향후 연구 방향

DeepGEMM의 기술은 향후 다음과 같은 분야에 영향을 미칠 것으로 예상됩니다:

1. 더 큰 AI 모델 학습 및 배포: 현재보다 더 큰 규모의 모델 개발 가능
2. 실시간 AI 서비스 향상: 더 빠른 응답 시간으로 사용자 경험 개선
3. 에지 디바이스에서의 AI 구현: 제한된 자원에서도 고성능 AI 구현 가능

추가적으로 향후 연구 방향은 다음과 같습니다:  
✅ FP8 연산의 정밀도 개선  
✅ MoE 모델 최적화 확대  
✅ PyTorch 및 Triton과의 통합 지원  

## 🎯 결론: NVIDIA Hopper GPU에서 FP8 연산을 극대화하는 DeepGEMM
✅ DeepGEMM은 NVIDIA의 최신 GPU에서 인공지능 모델의 핵심 연산인 행렬 곱셈을 극도로 최적화한 소프트웨어입니다. 8비트 부동소수점(FP8) 연산을 활용해 기존보다 최대 2.7배 빠른 속도를 달성했습니다.  
✅ 쉽게 말해, DeepGEMM은 "AI 모델의 엔진을 업그레이드" 하는 것과 같습니다. 같은 하드웨어로 더 빠르게, 더 효율적으로 AI를 구동할 수 있게 해주는 중요한 기술 발전입니다.  
✅ 이 라이브러리는 특히 ChatGPT, Claude와 같은 대형 언어 모델이나 Mix-of-Experts 구조를 사용하는 최신 AI 모델에서 큰 성능 향상을 가져올 수 있습니다.