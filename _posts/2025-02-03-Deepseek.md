---
layout: post
title:  "DeepSeek에서 DeepSeek 물어보기"
summary: "중국에서 개발한 LLM 모델"
date:   2025-02-02 16:00 -0400
categories: AI
math: true
---

- **공식 웹사이트**: [https://www.deepseek.com/](https://www.deepseek.com/)
- **GitHub**: [https://github.com/deepseek-ai](https://github.com/deepseek-ai)
- **논문 리스트**:
  + [DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models](https://arxiv.org/abs/2401.06066)
  + [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://arxiv.org/abs/2402.03300)

*본 분석은 DeepSeek의 오픈소스 모델과 관련 논문을 바탕으로 작성되었습니다.*

---

## DeepSeek 란?



![1](/assets/img/post_img/deepseek/deepseek.png)



DeepSeek는 2023년에 설립된 중국의 인공지능 기업으로, "AGI(인공지능 일반화)를 현실로 만든다"는 목표 아래 다양한 대규모 언어 모델(LLM)을 개발합니다. 여기서 언어 모델은 사람이 작성한 글이나 질문을 읽고 적절한 답변이나 텍스트를 생성하는 인공지능 프로그램을 의미합니다. DeepSeek는 다음 두 가지 큰 특징으로 주목받고 있습니다.

- 오픈소스 접근 방식: DeepSeek는 자신들의 모델과 코드를 공개하여 누구나 사용하고, 수정하며, 발전시킬 수 있게 합니다. 이것은 개발자와 연구자들이 쉽게 접근하고, 개선점을 공유할 수 있게 하여 AI 기술의 민주화를 촉진합니다.

- 비용 효율성: 전통적인 대규모 언어 모델은 막대한 계산 자원과 비용이 필요하지만, DeepSeek의 모델은 기존보다 훨씬 적은 비용과 연산으로 높은 성능을 발휘하도록 설계되었습니다. 예를 들어, DeepSeek의 모델은 미국의 일부 고비용 모델보다 약 1/10 수준의 비용으로 개발되었다고 알려져 있습니다.



![1](/assets/img/post_img/deepseek/table.png)



---

## 1. MoE(Mixture of Experts) 아키텍처

MoE는 여러 “전문가(Expert)”들로 구성된 네트워크입니다. 각각의 전문가는 특정 문제에 대해 뛰어난 해결 능력을 가지고 있습니다. 기존 MoE 모델은 모든 전문가를 동시에 사용하여 계산 비용이 높고, 일부 전문가에 너무 많은 작업이 몰리는 문제가 있었습니다. DeepSeek는 아래와 같이 문제를 해결합니다.

- Sparse Activation: 입력된 데이터에 맞게 2~4개의 전문가만 선택하여 사용합니다. 이렇게 하면 불필요한 계산을 줄이고 빠른 응답을 얻을 수 있습니다.

- Expert Balancing Algorithm: 각 전문가가 골고루 사용되도록 조정하여, 특정 전문가에 작업이 과도하게 몰리는 것을 방지합니다.

이 두 가지 개선을 통해, 같은 규모의 모델에서도 더 높은 성능을 내면서도 학습 및 추론 비용을 크게 줄일 수 있습니다.

## 2. Hybrid Training 전략

- 초기 학습(Pre-training):
 + 목적: 모델이 언어의 기본 규칙과 일반적인 지식을 습득하도록 합니다.
 + 방법: 위키피디아, 책, 웹 문서 등 다양한 일반 텍스트 데이터를 사용해 대규모로 학습합니다.
 + 결과: 모델은 문맥 이해와 문장 생성 같은 기본 능력을 갖추게 됩니다.

- 도메인 특화(Fine-tuning):
 + 목적: 특정 분야(예: 수학, 코드, 과학)에 대해 더욱 정밀하고 전문적인 능력을 갖추도록 합니다.
 + 방법: 해당 분야의 데이터를 추가로 학습시킵니다. 예를 들어, DeepSeekMath 모델은 수학 문제를 단계별로 풀어가는 “Chain-of-Thought” 방식으로 학습되어, 복잡한 수학 문제 해결 능력이 크게 향상됩니다.
 + 결과: 모델은 특정 도메인에서 보다 높은 정확도와 전문성을 발휘할 수 있습니다.

## 3. 연산 자원 최적화

DeepSeek는 한정된 컴퓨팅 자원으로도 큰 모델을 효과적으로 학습할 수 있도록 여러 기술을 도입했습니다.

- FlashAttention 활용: GPU의 메모리 사용량을 최적화하여, 긴 문맥(많은 단어)을 빠르게 처리할 수 있게 합니다.

- 양자화(Quantization) 지원: 모델의 파라미터를 4비트 또는 8비트로 압축함으로써, 엣지 디바이스(예: 스마트폰, 임베디드 시스템)에서도 모델을 실행할 수 있도록 해줍니다. 이렇게 하면 필요한 연산량과 메모리 소비가 대폭 줄어들어 비용 효율성이 높아집니다.

---

## DeepSeek의 모델 종류

- DeepSeek-LLM: 7B와 67B 파라미터 버전으로 제공되며, 일반 텍스트 생성, 대화, 정보 검색 등 폭넓은 응용 분야에서 활용됩니다.

- DeepSeek-MoE: 앞서 설명한 MoE 아키텍처를 적용하여 연산 효율성을 극대화한 모델로, 높은 확장성과 저비용 학습이 가능한 장점을 가지고 있습니다.

- DeepSeekMath: 수학 문제 해결 및 수학적 추론에 특화된 모델로, "Chain-of-Thought" 기법을 적용하여 복잡한 수학 문제에 대해 높은 정확도를 기록합니다.

- DeepSeek-R1: 강화 학습(RL) 기반 모델로, 논리적 문제 해결, 코딩, 수학 문제 등에서 뛰어난 성능을 보입니다. 특히, R1 모델은 "생각을 소리내어 표현"하는 체인-오브-토트(Chain-of-Thought) 기능을 통해 사용자에게 모델의 추론 과정을 투명하게 제공하여 신뢰도를 높입니다.

---

## 영향력 및 미래 전망

- 시장 재평가: DeepSeek-R1 출시 이후, 미국의 주요 기술주(예: Nvidia, Microsoft 등)가 큰 폭으로 하락하며 AI 인프라 비용에 대한 재평가가 이루어졌습니다.

- 비용 효율성의 새로운 패러다임: 오픈소스 기반으로 개발된 DeepSeek의 접근 방식은 향후 AI 기술이 ‘고가의 첨단 모델’과 ‘저렴한 범용 모델’으로 이원화될 가능성을 제시합니다. 이는 기존 미국 중심의 AI 투자 패러다임에 도전장을 내미는 중요한 요소로 평가됩니다.

- 국제 경쟁과 기술 민주화: 중국 정부와 주요 언론은 DeepSeek를 국가 기술 자산으로 평가하며, AI 분야에서의 글로벌 경쟁 구도를 재편할 중요한 계기로 보고 있습니다. 오픈소스와 비용 절감이라는 강점을 바탕으로 DeepSeek는 앞으로 AI 생태계의 민주화에 기여할 것으로 기대됩니다.

---

## 결론
DeepSeek는 혁신적인 MoE 아키텍처, 하이브리드 학습 전략, 연산 자원 최적화 기술, 그리고 오픈소스 생태계를 결합하여 높은 성능과 비용 효율성을 동시에 달성한 차세대 대규모 언어 모델을 개발하고 있습니다. 이러한 기술적 접근은 DeepSeek가 전 세계 개발자와 연구자들에게 영감을 주며, 향후 AI 기술 발전과 글로벌 시장 재편에 중추적인 역할을 할 것으로 기대됩니다.
