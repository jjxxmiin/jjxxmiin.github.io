---
layout: post
title:  "cycleGAN-tensorflow"
summary: "cycleGAN 구현하기"
date:   2019-02-28 09:00 -0400
categories: gan
---
[깃허브](https://github.com/jjxxmiin/my_gan/tree/master/cyclegan)

# requirement
- python 3
- scipy
- numpy
- tqdm
- tensorflow

---

# dataset
- [https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/](https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/)

---

# File
- ops
- utils
- CycleGAN
- train

---

# 개념
## Residual block
- skip connection과 비슷하다.
- ResNet

```
x -> conv -> norm -> relu -> conv -> norm -> y
return x + y
```

## instance normalization



![input_norm](/assets/img/post_img/cyclegan/instance_norm.PNG)



- **채널단위** 로 normalization

## LSGAN
loss 함수의 차이
- 기존 : sigmoid cross entropy loss function
  + -> discriminator를 속이기만 하면 된다.
- **LSGAN** : least square loss function
  + -> real input data와 근접해야 한다.

## auto encoder
비감독학습(UnSupervised learning) 데이터의 특징(흥미로운 구조)을 발견하기 위함
- 입력을 압축하고 재구성한다.
- 주관적으로 중요한 정보들을 최대한 보존하는 것을 목표로 하며, 이를 위해 사람들이 잘 느끼지 못하거나 둔감한 정보는 손실시킨다.

## identity loss
이것은 X에서 Y'로 Y에서 X'로 generate를 할 때 Loss에다가 Y'에서 Y로 X'에서 X로 가는 Mapping Function의 Loss를 추가한 것이다.
- X',Y'의 특징을 유의미하게 만든다.

---

# CycleGAN
- **Unpaired** Image-to-Image Translation using Cycle-Consistent Adversarial Networks



![model](/assets/img/post_img/cyclegan/model.PNG)



## hyperparameter
```python
img_ch = 3
img_size = 256
learning_rate = 0.0002

# feature weight
gan_w = 1.0 # X -> Y'            1 * loss(Y,Y')
cycle_w = 10.0 # X -> Y' -> X''  10 * loss(X,X'')
identity_w = 5.0 # Y -> X'       5 * loss(X,X')

epoch = 2
iteration = 100000
batch_size = 1
```

- loss함수에 gan_w,cycle_w,identity_w를 각각 곱해주었다.
- cycle -> identity -> gan 순서로 feature의 중요도를 잡아주었다.

## generate
- conv 7x7
  - instance_norm
  - relu

---

- conv 3x3
  - instance_norm
  - relu

---

- conv 3x3
  - instance_norm
  - relu

---

- residual block
- residual block
- residual block
- residual block
- residual block
- residual block

---

- conv_transpose 3x3
  - instance_norm
  - relu

---

- conv_transpose 3x3
  - instance_norm
  - relu

---

- conv 7x7
  - tanh

## discriminate
- conv 4x4
  - leaky_relu

---

- conv 4x4
  - instance_norm
  - leaky_relu

---

- conv 4x4
  - instance_norm
  - leaky_relu

---

- conv 4x4
  - instance_norm
  - leaky_relu

---

- conv 4x4

**LSGAN LOSS를 이용하기 때문에 sigmoid를 쓸 필요없다.**


## loss
- L1

```
loss = tf.reduce_mean(tf.abs(x - y))
```

- G_LOSS

```python
#LSGAN
fake_cost = tf.reduce_mean(tf.squared_difference(fake, 1.0))

return fake_cost
```

- D_LOSS

```python
#LSGAN
real_cost = tf.reduce_mean(tf.squared_difference(real,1.0))
fake_cost = tf.reduce_mean(tf.square(fake))

return real_cost + fake_cost
```

- LOSS

```python
# loss
# X' -> X
# Y' -> Y
identity_loss_A = L1_loss(G_aa,train_A)
identity_loss_B = L1_loss(G_bb,train_B)

# X -> Y'
# Y -> X'
G_cost_A = g_cost(D_fake_A)
G_cost_B = g_cost(D_fake_B)

# X -> Y' vs real
D_cost_A = d_cost(D_real_A,D_fake_A)
D_cost_B = d_cost(D_real_B,D_fake_B)

# X -> Y' -> X'' vs X
recon_cost_A = L1_loss(G_aba,train_A)
recon_cost_B = L1_loss(G_bab,train_B)

G_loss_A = gan_w * G_cost_A + \
           cycle_w * recon_cost_A + \
           identity_w * identity_loss_A

G_loss_B = gan_w * G_cost_B + \
           cycle_w * recon_cost_B + \
           identity_w * identity_loss_B

D_loss_A = gan_w * D_cost_A
D_loss_B = gan_w * D_cost_B

G_loss = G_loss_A + G_loss_B
D_loss = D_loss_A + D_loss_B

```

---

# 참조
- [https://www.slideshare.net/NaverEngineering/1-gangenerative-adversarial-network](https://www.slideshare.net/NaverEngineering/1-gangenerative-adversarial-network)
- [http://www.kwangsiklee.com/2018/03/cyclegan%EC%9D%B4-%EB%AC%B4%EC%97%87%EC%9D%B8%EC%A7%80-%EC%95%8C%EC%95%84%EB%B3%B4%EC%9E%90/](http://www.kwangsiklee.com/2018/03/cyclegan%EC%9D%B4-%EB%AC%B4%EC%97%87%EC%9D%B8%EC%A7%80-%EC%95%8C%EC%95%84%EB%B3%B4%EC%9E%90/)
- [https://github.com/taki0112/CycleGAN-Tensorflow](https://github.com/taki0112/CycleGAN-Tensorflow)
- [https://artoria.us/22](https://artoria.us/22)
- [http://jaejunyoo.blogspot.com/2017/04/lsgan-2.html](http://jaejunyoo.blogspot.com/2017/04/lsgan-2.html)
- [https://blog.lunit.io/2018/04/12/group-normalization/](https://blog.lunit.io/2018/04/12/group-normalization/)
- [https://m.blog.naver.com/laonple/220793640991](https://m.blog.naver.com/laonple/220793640991)
