---
layout: post
title: '[2026-01-23] LoL (Longer than Longer): 12시간 무한 비디오 생성의 시대를 여는 RoPE Jitter
  기술 심층 분석'
date: '2026-02-01'
categories: tech
math: true
summary: 12시간 이상의 무한 비디오 생성을 실현한 LoL 프레임워크와 Sink-Collapse 해결 전략
image:
  path: https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2601.16914.png
  alt: Paper Thumbnail
---

## 1. Executive Summary (핵심 요약)

최근 비디오 생성 AI 분야는 단기적인 영상미를 넘어, 장시간의 일관성을 유지하며 수 분, 혹은 수 시간 단위의 영상을 생성하는 방향으로 진화하고 있습니다. 본 분석에서 다룰 연구인 **"LoL: Longer than Longer, Scaling Video Generation to Hour"**는 기존 자기회귀(Autoregressive) 기반 비디오 생성 모델들이 겪어온 치명적인 한계인 '오류 누적(Error Accumulation)'과 '싱크 붕괴(Sink-Collapse)' 현상을 해결함으로써, 세계 최초로 **12시간 이상의 초장기 비디오 생성**을 실현했습니다.

연구진은 'Attention Sink' 메커니즘을 적용할 때 발생하는 특정 패턴의 반복과 장면 초기화 현상이 **Rotary Position Embedding(RoPE)**의 주기적 구조와 **Multi-head Attention(MHA)** 간의 상호작용 부조화에서 기인함을 수학적으로 증명했습니다. 이를 해결하기 위해 제안된 **Multi-head RoPE Jitter**는 별도의 추가 학습이 필요 없는(Training-free) 경량 알고리즘으로, 각 어텐션 헤드의 위상을 의도적으로 분산시켜 어텐션 동질화(Homogenization)를 방지합니다. 본 보고서에서는 LoL 프레임워크의 기술적 아키텍처와 실제 산업적 파급력, 그리고 한계점에 대해 심층적으로 고찰합니다.

## 2. Introduction & Problem Statement (연구 배경 및 문제 정의)

Sora와 같은 대형 비디오 모델의 등장 이후, 생성 AI의 다음 전장은 '길이'와 '일관성'이 되었습니다. 초기 비디오 생성 모델들은 양방향(Bidirectional) 어텐션을 사용하여 전체 프레임을 동시에 생성했으나, 이는 메모리 소모가 기하급수적으로 늘어나는 $O(N^2)$의 복잡도를 가져 장편 영상 제작에 부적합했습니다. 이에 대한 대안으로 최근에는 프레임을 순차적으로 생성하는 **자기회귀적(Autoregressive) 방식**이 주류로 자리 잡았습니다.

하지만 자기회귀 방식 역시 두 가지 큰 장벽에 부딪혔습니다:
1.  **컨텍스트 윈도우의 한계**: 무한히 늘어나는 KV 캐시(Key-Value Cache)를 감당할 수 없어 이전 정보를 버려야 함.
2.  **싱크 붕괴(Sink-Collapse)**: 성능 유지를 위해 초기 프레임(Attention Sink)을 고정해 두었을 때, 특정 시점부터 생성된 화면이 갑자기 초기 프레임으로 되돌아가거나(Scene Reset) 움직임이 순환(Cyclic Motion)되는 현상.

연구진은 특히 이 '싱크 붕괴' 현상에 주목했습니다. 이는 단순히 모델의 용량이 부족해서가 아니라, 위치 인코딩 방식인 RoPE가 긴 시간축 위에서 어텐션 헤드들을 특정 '싱크 프레임'에 과도하게 집중시키기 때문에 발생합니다. 즉, 모델이 새로운 내용을 생성해야 할 시점에 과거의 '닻(Anchor)'에 묶여버리는 일종의 지능적 고착 현상인 것입니다.

## 3. Core Methodology (핵심 기술 및 아키텍처 심층 분석)

### 3.1. Attention Sink와 문제의 근원

Streaming LLM 등에서 영감을 받은 Attention Sink 기법은 전체 KV 캐시 중 가장 첫 몇 프레임과 가장 최근의 몇 프레임만을 유지하는 방식입니다. 이론적으로는 효율적이지만, 비디오 생성에서는 이 '싱크 프레임'이 강력한 어트랙터(Attractor)로 작용하여 모든 어텐션 헤드가 동시에 싱크 프레임을 참조하게 만듭니다. 이를 **Inter-head Attention Homogenization(헤드 간 어텐션 동질화)**라고 합니다.

![Visualization of inter-head attention homogenization.](/assets/img/papers/2601.16914/x2.png)
*Figure 3: 어텐션 헤드 간 동질화 현상 시각화. 하단 두 줄에서 보듯, 특정 시점에 여러 헤드가 동시에 싱크 프레임에 과도한 가중치를 할당하며 장면 붕괴가 일어납니다.*

### 3.2. RoPE의 주기성과 위상 충돌

RoPE(Rotary Position Embedding)는 토큰의 위치 정보를 복소 평면상의 회전으로 표현합니다. 각 차원마다 회전 주기가 다른데, 매우 긴 시퀀스에서는 이 주기가 겹치는 지점이 발생합니다. 연구진의 분석에 따르면, 특정 타임스텝 $t$에서 서로 다른 어텐션 헤드들이 RoPE의 주기적 성질로 인해 동일한 위치적 편향을 가지게 되며, 이것이 싱크 프레임에 대한 '집단적 과몰입'을 유도합니다.

### 3.3. Multi-head RoPE Jitter

이 문제를 해결하기 위해 LoL이 제안한 핵심 해결책은 매우 우아합니다. 바로 각 어텐션 헤드마다 RoPE의 베이스 주파수나 위상에 미세한 무작위 노이즈(Jitter)를 추가하는 것입니다.

$$\theta_{j,h} = \theta \cdot C^{-2j/d} + \epsilon_h$$

여기서 $\epsilon_h$는 헤드 $h$에 고유하게 부여되는 지터 값입니다. 이 작은 변화가 각 헤드가 바라보는 '위치적 지도'를 미세하게 어긋나게 만듭니다. 결과적으로 모든 헤드가 동시에 싱크 프레임에 함몰되는 것을 방지하고, 각 헤드가 시공간적 맥락의 서로 다른 부분을 참조하도록 강제합니다. 이는 생물학적 시스템에서 뉴런들이 서로 다른 위상으로 발화하여 정보 처리 효율을 높이는 것과 유사한 원리입니다.

## 4. Implementation Details & Experiment Setup (구현 및 실험 환경)

LoL 프레임워크는 **Lumina-Next-T2V**와 같은 Diffusion Transformer(DiT) 구조를 베이스 모델로 채택했습니다. 

*   **KV 캐시 관리**: 고정된 크기의 슬라이딩 윈도우 KV 캐시를 사용하며, 첫 3~5프레임을 Attention Sink로 고정합니다.
*   **추론 최적화**: 12시간 분량의 영상을 생성하기 위해 실시간 스트리밍 추론 파이프라인을 구축했습니다. 이는 생성된 프레임을 즉시 인코딩하여 저장하고 메모리에서 비우는 방식으로 진행됩니다.
*   **데이터셋**: 고해상도 시네마틱 영상 데이터를 활용하여 긴 시간 동안의 텍스처 일관성을 학습한 체크포인트를 사용했습니다.

이 방식의 가장 큰 장점은 **학습 단계가 전혀 필요 없다**는 것입니다. 기존에 잘 학습된 DiT 모델에 추론 시점에만 RoPE Jitter 코드를 몇 줄 추가하는 것만으로 12시간 연속 생성이 가능해집니다.

## 5. Comparative Analysis (성능 평가 및 비교)

실험 결과는 놀랍습니다. 기존의 최첨단(SOTA) 모델들은 약 1,000프레임(수 분 내외)을 기점으로 급격한 화질 저하나 장면 초기화 현상을 보였습니다. 반면, LoL은 12시간(약 1,296,000 프레임 이상) 동안 생성 품질의 유의미한 하락 없이 안정적인 출력을 유지했습니다.

![Streamingly generated ultra long video (12 hours) for prompt “A cinematic third-person shot of a wingsuit flyer racing through a narrow mountain valley.”](/assets/img/papers/2601.16914/x1.png)
*Figure 1: 12시간 동안 생성된 윙수트 플라이어 영상의 스틸컷. 시간이 지나도 지형의 논리적 구조와 물리적 움직임이 유지됩니다.*

### 5.1. 정량적 지표
*   **FVD (Fréchet Video Distance)**: 장기 생성 시에도 FVD 수치가 안정적으로 유지됨을 확인했습니다.
*   **Temporal Consistency Score**: 프레임 간 흐름의 매끄러움을 측정했을 때, RoPE Jitter 적용 시 점수가 약 25% 향상되었습니다.

## 6. Real-World Application & Impact (실제 적용 분야 및 글로벌 파급력)

LoL의 '무한 생성' 능력은 단순히 기술적 과시를 넘어 산업 전반에 파괴적 혁신을 불러올 수 있습니다.

1.  **미디어 및 엔터테인먼트**: '끊이지 않는 배경 영화(Ambient Cinema)'나 AI 기반의 24시간 실시간 방송 생성이 가능해집니다. 시청자의 상호작용에 따라 전개가 무한히 확장되는 게임적 요소가 결합된 영화의 탄생을 예고합니다.
2.  **디지털 트윈 및 시뮬레이션**: 자율주행차 학습을 위해 며칠 밤낮을 지속하는 주행 시나리오를 가상으로 생성할 수 있습니다. 이는 현실 세계에서 수집하기 어려운 롱테일(Long-tail) 데이터를 무한히 확보하는 수단이 됩니다.
3.  **메타버스 가상 환경**: 고정된 루프 영상이 아닌, 매 순간 미세하게 변화하는 실시간 풍경을 저비용으로 렌더링할 수 있습니다.
4.  **경제적 가치**: 기존 모델은 긴 영상을 위해 수많은 분할 생성과 편집 과정이 필요했으나, LoL은 이를 단일 파이프라인으로 통합하여 제작 비용을 90% 이상 절감할 수 있는 잠재력을 지닙니다.

## 7. Discussion: Limitations & Critical Critique (한계점 및 기술적 비평)

하지만 이 연구를 전적으로 완벽하다고 보기는 어렵습니다. 시니어 과학자로서 다음과 같은 비판적 시각을 제시합니다.

첫째, **'의미론적 표류(Semantic Drift)'** 문제입니다. LoL은 시각적인 싱크 붕괴는 막았지만, 12시간 동안 생성된 영상이 초기에 주어진 프롬프트의 '서사적 의도'를 얼마나 엄격하게 유지하는지는 의문입니다. 시각적 연속성은 유지될지언정, 이야기가 산으로 가는 현상을 제어할 수 있는 상위 수준의 플래닝 메커니즘이 부족합니다.

둘째, **RoPE Jitter의 이론적 근거**에 대한 보완이 필요합니다. 무작위 노이즈가 우연히 헤드 간 동질화를 막아주는 것은 확인되었으나, '최적의 지터 분포'가 무엇인지에 대한 엄밀한 수학적 정의는 아직 부족합니다. 이는 특정 도메인의 영상에서는 지터가 오히려 아티팩트(Artifact)를 발생시킬 위험이 있음을 시사합니다.

셋째, **하드웨어 자원 효율성**입니다. 아무리 스트리밍 방식이라 하더라도 12시간 동안 GPU를 가동하는 비용은 상당합니다. 추론 효율성을 극대화하기 위한 모델 경량화나 양자화 기술과의 결합 연구가 병행되어야 할 것입니다.

## 8. Conclusion (결론 및 인사이트)

"LoL: Longer than Longer" 연구는 비디오 생성 AI의 패러다임을 '단편 제작'에서 '영속적 스트리밍'으로 전환한 기념비적인 작업입니다. 복잡한 재학습 대신, 아키텍처 내부의 수학적 특성을 깊이 파고들어 RoPE Jitter라는 단순하고도 강력한 해결책을 제시한 점은 엔지니어링 측면에서 매우 고무적입니다.

이제 AI는 단순히 이미지를 그려주는 도구를 넘어, 독자적인 시공간을 지속적으로 창조하는 '월드 시뮬레이터(World Simulator)'로 진화하고 있습니다. 향후 이 기술이 긴 문맥을 이해하는 LLM의 추론 능력과 결합된다면, 우리는 인간의 개입 없이도 완결성 있는 서사를 갖춘 장편 영화를 실시간으로 생성하고 소비하는 시대를 맞이하게 될 것입니다. LoL은 그 무한한 가능성을 향한 첫 번째 이정표입니다.

[Original Paper Link](https://huggingface.co/papers/2601.16914)