---
layout: post
title:  "Coral USB 사용하기"
summary: "Coral USB를 이용해 모델 돌리기"
date:   2019-07-05 13:00 -0400
categories: pi
---

# Dependency
- raspberry pi 3
- Google Coral USB



# TensorFlow 설치하기

```
$ pip install --upgrade  "tensorflow==1.7.*"

$ pip install PILLOW
```

# 라이브러리 설치

```
$ wget http://storage.googleapis.com/cloud-iot-edge-pretrained-models/edgetpu_api.tar.gz
$ tar xzf edgetpu_api.tar.gz
$ cd python-tflite-source
```

```
$ bash ./install.sh
```

# CORAL USB 연결하기
- 기존에 연결이 되어있는 상태로 진행했다면 다시 빼고 연결해야 한다.

# 예제 이미지 다운로드

```
cd ~/Downloads/
```
```
wget https://dl.google.com/coral/canned_models/mobilenet_v2_1.0_224_inat_bird_quant_edgetpu.tflite \
https://dl.google.com/coral/canned_models/inat_bird_labels.txt \
https://coral.withgoogle.com/static/docs/images/parrot.jpg
```

# 예제 실행

```
cd python-tflite-source/edgetpu/demo
```
```
python3 classify_image.py \
--model ~/Downloads/mobilenet_v2_1.0_224_inat_bird_quant_edgetpu.tflite \
--label ~/Downloads/inat_bird_labels.txt \
--image ~/Downloads/parrot.jpg
```

---

# 모델
- [[HERE](https://coral.withgoogle.com/models/)]

# 사용자 예제 코드

- 출처 : [https://www.pyimagesearch.com/2019/05/13/object-detection-and-image-classification-with-google-coral-usb-accelerator/](https://www.pyimagesearch.com/2019/05/13/object-detection-and-image-classification-with-google-coral-usb-accelerator/)

```python
from edgetpu.detection.engine import DetectionEngine
from imutils.video import VideoStream
from PIL import Image
import argparse
import imutils
import time
import cv2

ap = argparse.ArgumentParser()
ap.add_argument("-m", "--model", required=True,
	help="path to TensorFlow Lite object detection model")
ap.add_argument("-l", "--labels", required=True,
	help="path to labels file")
ap.add_argument("-c", "--confidence", type=float, default=0.3,
	help="minimum probability to filter weak detections")
args = vars(ap.parse_args())

labels = {}

for row in open(args["labels"]):
	(classID, label) = row.strip().split(maxsplit=1)
	labels[int(classID)] = label.strip()

model = DetectionEngine(args["model"])

print("[INFO] starting video stream...")
cam = VideoStream(src=0).start()
time.sleep(2.0)

# loop over the frames from the video stream
while True:
	# grab the frame from the threaded video stream and resize it
	# to have a maximum width of 500 pixels
	frame = cam.read()
	frame = imutils.resize(frame, width=500)
	ori = frame.copy()

	# prepare the frame for object detection by converting (1) it
	# from BGR to RGB channel ordering and then (2) from a NumPy
	# array to PIL image format
	frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
	frame = Image.fromarray(frame)

	# make predictions on the input frame
	start = time.time()
	results = model.DetectWithImage(frame, threshold=args["confidence"],
		keep_aspect_ratio=True, relative_coord=False)
	end = time.time()

	# loop over the results
	for r in results:
		# extract the bounding box and box and predicted class label
		box = r.bounding_box.flatten().astype("int")
		(startX, startY, endX, endY) = box
		label = labels[r.label_id]

		# draw the bounding box and label on the image
		cv2.rectangle(ori, (startX, startY), (endX, endY),
			(0, 255, 0), 2)
		y = startY - 15 if startY - 15 > 15 else startY + 15
		text = "{}: {:.2f}%".format(label, r.score * 100)
		cv2.putText(ori, text, (startX, y),
			cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

	# show the output frame and wait for a key press
	cv2.imshow("Frame", ori)
	key = cv2.waitKey(1) & 0xFF

	# if the `q` key was pressed, break from the loop
	if key == ord("q"):
		break

# do a bit of cleanup
cv2.destroyAllWindows()
cam.stop()

```

```
python detect_image.py --model model_name.tflite
	--labels labels.txt
```


# 참조
- [https://www.pyimagesearch.com/2019/05/13/object-detection-and-image-classification-with-google-coral-usb-accelerator/](https://www.pyimagesearch.com/2019/05/13/object-detection-and-image-classification-with-google-coral-usb-accelerator/)

- [https://geeksvoyage.com/raspberry%20pi/google-coral-init/](https://geeksvoyage.com/raspberry%20pi/google-coral-init/)
