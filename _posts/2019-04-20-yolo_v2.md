---
layout: post
title:  "YOLOv2,YOLO 9000"
summary: "YOLO 9000 논문 읽어보기"
date:   2019-04-20 13:00 -0400
categories: paper
---

# YOLO 9000

- YOLO 9000 Paper : [Here](https://arxiv.org/abs/1612.08242)

---

# YOLO가 빠른 이유
bounding box의 위치를 찾는 것과 classification이 동시에 이루어 지기 때문에 속도가 빠르다. 반면에 R-CNN 계열의 검출 네트워크들은 영상에서 오브젝트가 있을 것 같은 후보(ROI - Region Of Interest : 관심영역) 를 먼저 뽑는다. 후보로 뽑힌 ROI(영상의 작은 부분) 들은 Classification network(분류기)에 의해 클래스 분류가 이뤄지고  그 후 bounding box 를 찾는다.

---
# 기존 YOLO의 문제점
1. 낮은 검출율(recall)
2. 상당한 수의 localization error
---

# YOLO 9000

# Abstract
9000가지 이상의 object detection을 수행할 수 있는 YOLOv2를 소개한다. PASCAL VOC와 COCO와 같은 표준 detection 작업에 대한 모델이다. 혁신적 multi-scale train 방법을 사용하면 속도와 정확도의 균형을 잘 유지 할 수 있다. 67FPS에서 YOLOv2는 76.8 mAP을 얻었고 40FPS에서는 78.6 mAP를 얻었다. 마지막으로 YOLO는 object detection과 classification에 대해서 jointly train(공동 훈련) 방법을 제안한다. 이 방법을 사용하여 YOLO는 9000 개 이상의 서로 다른 object의 class를 예측한다. 그리고 그것은 여전히 **실시간** 으로 진행된다.

---

# Introduction
일반적인 object detector은 빠르고,정확하고,다양한 물체를 인식할 수 있어야한다. Neural Net의 도입 이후 프레임 워크는 점점 더 빠르고 정확해졌다. 그러나 대부분의 detection 방법은 여전히 작은 object 집합에 제약을 받는다. 현재의 object detection 데이터 세트는 classification과 tagging 같은 다른작업을 위한 데이터 세트와 비교해 제한되어 있다. 가장 일반적인 detection 데이터 세트는 수천 수십만 개의 이미지를 포함한다. 우리는 detection이 object classification의 수준으로 확장되기를 원한다. 그러나 detection용 image labelling은 classification또는 tagging을 위한 labelling보다 expensive하다. YOLO는 이미 보유하고 있는 대량의 classification 데이터를 활용하여 현재 detection의 범위를 확장하기 위한 새로운 방법을 제안한다. 그 방법은 서로 다른 데이터 세트를 함께 결합할 수 있는 object classification의 **계층적 관점** 을 사용한다. 또한 detection 과 classification 데이터에 object detector을 훈련시킬 수 있는 joint training 알고리즘을 제안한다. YOLO의 방법은 분류된 검출 이미지를 활용해서 object의 위치를 정확하게 파악하는 동안 어휘와 견고함을 향상시키는 방법을 학습한다. 이 방법을 이용해서 9000가지가 넘는 object 카테고리를 detection 할 수 있다.

- 사전 훈련된 모델 : [HERE](http://pjreddie.com/yolo9000/)

---

# Better
YOLO는 최첨단 detection 시스템에 비해 다양한 단점을 가지고 있다. Fast R-CNN에 비해 YOLO는 상당한 수의 위치 파악 오류를 만든다는 것을 보여준다. 그래서 classification 정확성을 유지하면서 recall(검출율) 과 localization을 개선하는데 주력한다. 더 나은 성능은 대용량의 네트워크를 교육하거나 여러 모델을 함께 모으는 방법(앙상블)을 사용할 수 있지만 YOLOv2로 빠른 것을 유지하고 더 정확한 detector를 원한다. 네트워크를 확장하는 대신에, 네트워크를 단순화시키고 그 표현 쉽게 배울수 있도록 만든다.


## Batch Normalization
- 다른 형태의 정규화(regularzation)가 필요없게 하며 convergence를 크게 향상시킨다.
- YOLO의 모든 CNN에 batch noremalization를 추가하면 mAP에서 2% 이상의 개선 효과를 얻을 수 있다.
- 모델을 regularize하는데 도움이된다. 과적합(overfitting)없이 dropout을 제거해도 된다.

---

## High Resolution Classiﬁer
- 모든 최첨단 detection 방법은 ImageNet에서 pretrain된 분류자를 사용한다.

- AlexNet부터는 256x256보다 작은 입력 이미지를 처리하고 기존 YOLO는 classifier 네트워크를 224x224로 훈련시키고 detection을 위해 448로 해상도를 증가시킨다.

- YOLOv2는 먼저 ImageNet에서 10 epoch 동안 448x448 해상도로 classifier 네트워크를 fine-tuning 시킨다. 이것은 고해상도 입력 이미지에서 더 잘 작동하도록 filter를 조정할 네트워크 시간을 제공한다. 그리고 탐지 된 결과 네트워크를 fine-tuning 한다.

- 고해상도 classifier 네트워크는 거의 4% mAP가 증가했다.

---

## Convolutional With Anchor Boxes

- YOLO는 convolution 특징 추출기 위에 fully connected layer를 사용하여 bounding box를 예측한다. 직접적으로 좌표를 예측하는 대신 Faster R-CNN은 사전에 수작업한 bounding box들을 예측한다.

- Faster R-CNN안의 RPN(region proposal network) 모듈에서 convolution layer들을 사용함으로써 offsets과 anchor boxes에 대한 신뢰도를 예측한다. prediction layer가 convolution이기 때문에, RPN은 feature map에서 모든 위치에서 offsets을 예측한다. 좌표 대신 offsets을 예측하면 문제가 간단해지고 네트워크 학습을 더 쉽게 할수 있다.

- fully connected layers를 YOLO에서 제거하고 bounding box를 예측하기 위해 anchor boxes를 사용한다. 먼저 output의 convolution layer를 높은 해상도로 만들기 위해 하나의 pooling layer를 제거한다.

- 448x448 대신 416 입력 이미지에서 작동하기 위해 네트워크를 줄인다. -> 최종 feature map을 홀수x홀수로 만들기 위해서이다. -> 홀수 이므로 단일 center cell이 존재할 수 있다. 짝수는 center가 4개 이다. 특히 큰 object는 image의 중심을 차지 하는 경향이 있으므로 feature map의 center가 4개로 잡힌다면 object를 검출하는데 어려움이 생길수 있다.

- YOLO의 convolution layer는 image를 32배만큼 downsampling하므로 416 입력 이미지를 사용해 13x13의 feature map을 얻는다.

- anchor box를 사용하면서 공간 위치로부터의 class 예측 매커니즘도 분리시키고 그것이 object일 확률을 예측하며 classification 한다. 예측된 bounding box가 object일때 그것이 어떤 class인지를 예측하기 때문에 **조건부 확률** 이 된다. anchor box를 사용하면 정확성이 약간 감소하지만 천개 이상의 예측을 할 수 있다.

- no anchor box = `mAP : 69.5`,`recall(검출율) : 81%`
- anchor box = `mAP : 69.2`,`recall(검출율) : 88%`

---

## Dimension Cluster



![cluster](/assets/img/post_img/yolo/cluster.PNG)



YOLO에서 anchor box를 사용할 때 두 가지 문제점을 갖는다.

- 첫번째, box dimension을 손수 설정 해야하는 것이다. 이전에는 직접 설정 했지만 YOLO는 자동적으로 bounding box를 학습에 적용하기 위해 k-means Clustering 알고리즘을 사용한다.
만약 표준 k-means를 유클리드 거리(점과 점사이의 거리)에서 사용한다면, 큰 box는 작은 box보다 많은 에러가 발생한다. 그러나 box의 크기와 상관없이 좋은 IOU를 이끄는 것을 원한다. 그러므로 거리 행렬을 위에 다음 식을 사용한다.



![dm](/assets/img/post_img/yolo/dm.PNG)



- 위에 그림을 보면 알 수 있듯이 k의 다양한 값을 위해 k-means를 작동하고 가장 가까운 IOU 평균을 표시한다. k가 커지면 clustering결과와 label사이의 IOU가 커져서 recall이 좋아지지만 모델의 복잡도가 상승하는 trade-off 관계가 있기 때문에 k값으로 5를 선택했다.

- **우선 순위의 anchor box를 선택하는 Clustering 전략과 직접선택하는 anchor box를 비교해보면 전자가 더 좋은 결과가 나옵니다.**

---

## Direct location prediction
- anchor box를 사용하면 학습 초기에 모델이 불안정해지는 문제를 해결할 수 있었는데 학습이 불안정한 원인은 box의 좌표가 랜덤하게 예측되기 때문이다. RPN에서 네트워크는 tx와 ty를 예측하고 (x,y)의 중앙 좌표는 아래 식에 의해서 계산된다.



![dm](/assets/img/post_img/yolo/dlp.PNG)



- tx,ty는 convolution연산의 출력 값이다.

- 예를 들어, tx=1이라면 box를 오른쪽으로 이동시킬 것이고 tx=-1이라면 왼쪽으로 이동할 것이다. 이 공식은 제한되지 않는다. 이 공식은 제약이 없으므로 box를 예측 한 위치에 관계 없이 anchor box의 위치가 영상 어디에도 나타날 수 있다. -> **문제점**

- offsets을 예측하는 대신에 grid cell의 위치와 상대적인 위치 좌표들을 예측한다. x,y의 위치가 grid cell 내부에만 있도록 제약을 두었기 때문에 0~1의 값을 갖게된다. 그리고 예측할때는 logistic activation을 사용해서 0~1의 값을 갖게한다.

- 네트워크는 output인 feature map에서의 각각의 cell에서 5개의 bounding box 정보를 예측한다. 각 bounding box의 tx,ty,tw,th,to 5개의 좌표를 예측한다.



![dm](/assets/img/post_img/yolo/dlp2.PNG)



- `cx,cy` : 각 `grid cell`의 좌상단 끝에 offset
- `px,py,pw,ph` : 우선 순위 anchor box의 x,y,width,height
- `tx,ty,tw,th` : 예측한 bounding box 좌표
- `to` : object인가?
- `bx = sigmoid(tx) + cx`에서 `tx`가 0값을 가지면 `bx`는 중심 좌표 x
- `by = sigmoid(ty) + cy`에서 `ty`가 0값을 가지면 `bx`는 중심 좌표 y

즉, tx와 ty는 0의 값을 가지기를 원한다. 또한 `tw,th`도 학습과정에서 0에 가까운 값이 되기를 원한다.

- `bw = pw*e^(tw)`에서 `tw`가 0값을 가지면 `bw = pw`
- `bh = ph*e^(th)`에서 `th`가 0값을 가지면 `bh = ph`
- `b` : 최종 bounding box

x,y는 grid cell안에 중심점을 잡기위해서 sigmoid 함수를 거치고 w,h는 exp을 사용해 box의 scale을 조절한다. exp함수의 특성상 음수가 나와도 양수로 변환되고 양수면 더 큰 값이 나오게 하므로 학습에 좀더 긍정적인 영향을 미칠수 있다.

- 위치 예측 파라미터에 sigmoid 함수, 경계박스의 중심이 grid cell 안에 있어야한다는 제약이 있기에 안정적이게 학습하고, Dimension Cluster를 사용했을때 mAP가 5% 상승한다.



![dm](/assets/img/post_img/yolo/dlp3.PNG)



- `grid cell`에서 만들어낸 bounding box는 그 grid cell을 갖고 있어야한다.

---

## Fine-Grained Features

- YOLOv2는 13x13 feature map 을 예측한다. 큰 object에서 충분한 반면, 더 작은 object들의 localizing하는 것 보다 세밀한 특징들로부터 효과를 얻을수 있다.

- Faster R-CNN이나 SSD는 네트워크의 다양한 feature map에서 그들이 Proposal Network(PN)를 실행해 해결방안을 제시한다. YOLOv2는 26x26 해상도의 이전 layer의 기능을 가져오는 **passthrough** layer만 추가하므로 다르게 접근한다.

- passthrough layer는 ResNet의 identity mapping과 유사하게 고해상도 feature와 저해상도 feature를 연결한다. 26x26x512 feature map은 13x13x2048 feature map으로 변경하고, 기존의 feature들과 연결할수 있다. -> 1% 향상



![fg](/assets/img/post_img/yolo/fg.PNG)



---

## Multi Scale Trining

- 원래 YOLO는 448x448의 input을 사용한다. anchor box 추가하어 416x416으로 변경했고 모델은 convolution과 pooling 계층만을 사용하므로 layer는 조정할 수 있다. 즉, fully connected layer가 없으므로 input size가 어떤것이 들어와도 상관없다.

- YOLOv2는 다양한 크기의 이미지에서 동작하길 원한다. input 이미지의 size를 고치는것 대신에 모든 네트워크의 반복을 변경했다. 다양한 해상도로 학습하기 위해 10 batch 마다 학습데이터가 32의 배수로 resize 된다(320,352 ... 608). 따라서 가장 작을 때는 320x320이고 가장 클 때는 608x608이다.

- YOLO는 dimension과 지속적인 학습에 대한 네트워크의 크기를 resize한다. 이러한 체제는 네트워크가 다양한 입력 차원들에 잘 예측할 수 있도록 학습하며 같은 네트워크가 다른 해상도에서 검출을 예측할 수 있음을 보여준다.

- YOLOv2에서 낮은 해상도의 경우에는 보다 약하게 작동하지만 꽤 정확한 검출을 한다. 288x288에서 그것은 90FPS 속도로 작동하고 mAP는 거의 Fast R-CNN만큼 잘 나온다.

---

# Faster
YOLO는 정확한 검출을 원하면서도 빠르기를 원한다. 로보틱스 또는 자율 주행 자동차와 같은 대부분의 검출에 있어서 응용프로그램들은 속도가 느리다. 최대한의 성능을 얻기 위해서는 처음부터 빠르게 설계한다.

- 대부분의 detection framework는 특징 추출기에 기반이 되는 VGG-16에 의존한다. 그러나 VGG-16은 정확하고 강력하지만 불필요하게 복잡하다.

## Darknet-19
- 새로운 분류 모델을 제안한다.
- VGG와 유사하게 `3x3 filter` 를 사용하고 모든 pooling 단계 이후 channel 수를 2배로 한다. (global average pooling 사용)
- Batch Normalization 사용
- 19개의 convolution layer
- 5개의 maxpooling
- ImageNet에서 Top-1에서는 `72.9%` top-5에서는 `91.2%`의 정확도를 가진다.



![model](/assets/img/post_img/yolo/model.PNG)



## Training for classification
- ImageNet `1000개 class`
- `160 epoch`
- `stochastic gradient descent`(`learning late` = 0.1)
- `polynomial rate decay`  : 4의 거듭제곱
- `weight decay` : 0.00005 ,`momentum` : 0.9
- `standard data augmentation`
- `224x224` -> `448x448` `fine tuning`(`10epochs`)


## Training for detection
- 마지막 `convolution layer`를 제거하고 3x3x1024 `convolution layer`를 추가
- 그 뒤에 `1x1 convolution layer` 추가
- 5개 `bounding box` 좌표를 예측
- 각각 `20개 클래스` 예측
- `125 filter` =  25(5x5) + 100(5x20)
- `160 epochs`
- `weight decay` : 0.00005 ,`momentum` : 0.9

## 여기까지 YOLOv2고 뒤에 부터 YOLO9000

---

# Stronger
- YOLO는 classification과 detection 데이터를 공동으로 운현할 수 있는 메커니즘을 제공한다. class label만 있는 이미지를 사용해 발견할수 있는 범주의 수를 확장한다.

- classification datasets은 매우 넓은 범위의 라벨들을 가진다.(Norfolk terrier, Yorkshire terrier etc.) 그렇기에 이러한 라벨들을 병합하기 위한 일관적인 방법이 필요하다.

- 대부분의 classification 방법은 최종 확률 분포를 계산하기 위해 모든 가능한 범주에서 `softmax` 레이어를 사용한다. 소프트맥스를 사용함으로써 추정한 class는 상호 배타적이다. YOLO9000은 상호 배재적임을 추정하지 않는 datasets을 조합하기 위해서 multi label model을 사용한다.

## Hierachical classification
WordNet에서 가져온다. WordNet은 연결된 그래프로 구조가 지어진다. 언어는 복잡하기 때문에 tree는 아니다. 예를 들어 '개'는 '개의 종'과 '토종 동물' 두개 모두의 타입이며 둘다 WordNet에서 동의어 집합이다. 전체 그래프 구조를 사용하는 대신에, 우리는 ImageNet에서 개념들로부터 계층적 트리를 만들어 문제를 간단하게 했다.

- 많은 동의어 집합들은 그래프를 통하여 하나의 경로만을 가지기 때문에 모든 경로를 추가한다. 그리고 트리가 성장하도록 하는 경로들을 추가하거나 버리면서 개념들을 조사한다.

- 만약 개념이 루트에서 두 개의 경로들을 가지거나 하나의 경로를 가진다면 트리에서 3개의 가장자리들을 추가해야 할 것이고 다른 경우에는 우리가 선택한 보다 짧은 경로를 선택한다.

- 최종적 결론은 시각적 개념의 계층적 모델인 `WordTree`다. `WordTree`에서 classification을 수행하기 위해서 각 하의어의 확률에 대하여 모든 노드에서 조건부 확률을 구한다.



![hc1](/assets/img/post_img/yolo/hc1.PNG)






![hc2](/assets/img/post_img/yolo/hc2.PNG)



- 특정 노드 대한 절대적인 확률을 계산하려면 간단하게 루트 노드에서 특정노드까지의 조건부 확률을 모두 곱한다. Norfolk 인지 알기를 원하면 위에 처럼 계산한다.

- classification을 목적으로 모든 이미지가 객체를 포함한다고 가정한다. `Pr(object) = 1` 이 접근법을 확인하기 위해서 Darknet-19 모델을 이미지넷으로 만들어진 1000개의 워드트리에서 학습한다. 워드트리 1K를 만들기 위해서 중간노드를 추가하기에 1369개가 되고 실제 label을 루트까지 전파시킨다. 예를들어 Norfolk는 Wordnet에서 관련어인 '개','포유류' 등의 label도 얻게 된다.

- `71.9%`의 top-1정확도, `90.4%`의 top-5 정확도 369개의 개념이 추가함에도 불구하고 정확도가 약간 떨어졌고 이 방삭에서의 classification은 몇가지 이익을 가진다. 새롭거나 알려지지 않은 물체의 카테고리를 적절하게 감소시킨다. 예를 들어 개의 종류가 불분명하다면 그것은 여전히 개로 인식하고 하의어에서는 낮은 신뢰도가 전파될것이다.

- 검출기는 bounding box와 트리의 확률을 예측한다. 그 물체의 클래스를 예측하거나 몇가지의 임계점에 도달할 때 까지 트리가 계속 아래로 전파된다. 높은 신뢰도를 가지는 경로를 따라가며 threshold가 나올 때 까지 계속 내려간다.



![word](/assets/img/post_img/yolo/word.PNG)



## Dataset combination with WordTree

- coco와 ImageNet을 조합하기 위해 워드트리를 사용한다. 간단하게 데이터 셋에 카테고리들을 트리의 동의어들로 연결한다.

## Joint classification and detection



![tree](/assets/img/post_img/yolo/tree.PNG)



- coco와 ImageNet의 상위 9000개의 클래스를 조합해 데이터 세트를 생성한다. 또한 평가하기 위해 포함되지 않은 클래스를 추가한다.

- WordTree에는 9418개의 클래스가 있고 ImageNet은 매우 크기 때문에 coco를 oversampling하여 4:1비율만큼 키워 균형을 맞춘다.

- 출력의 크기를 제한하기위해 priors를 5개 대신 3개를 사용한다. 정상적인 loss를 backpropagate하고 classification loss를 위해 오직 label 수준 위로 backpropagate한다. 예를 들어 label이 '개'라면 정보를 가지고 있지 않기 때문에 tree아래쪽에 있는 '골든 리트리버'와 '독일 셰퍼드'에 어떤 오류를 할당한다. classification이미지를 봤을 때 오직 classification loss를 backpropagate하고 이것을 하기 위해서 간단하게 클래스에서 가장 높은 확률을 예측한 bounding box를 찾는다. 그리고 tree를 예측했을 때의 loss를 계산한다. bounding box와 ground truth가 겹치는 것이 0.3 IOU 이상일 경우에만 objectness loss를 역전파 하도록 했다.

- YOLO는 새로운 특정한 동물을 잘 학습하는 것을 보았고 **옷이나 장비 같은 카테고리들은 학습하지 못했다.**

---

# 결론
- YOLOv2는 빠르게 작동하고 속도와 정확도 사이의 tradeoff가 좋다.

- Hierachical classification를 사용해서 데이터 셋을 조합하는 것은 classification 과 segmentation에 효율적이다.

- 미래에 작업은 이미지 segmentation에서 weakly supervised를 통한 기술들을 사용하기를 희망한다.

---

# 추가 용어

## RPN(region proposals network)
- RPN은 지역을 제안하는 네트워크라는 의미로 sliding window에 anchor box를 제안하는 네트워크이다.

- image에서 cnn으로 feature map을 찾고 feature map을 input으로 RPN에서 미리 학습된 anchor box를 이용해 후보영역을 추출하고 classifier과 regression 네트워크를 거쳐서 object인지 background인지, anchor당 **델타값** 들(x,y,w,h)를 내놓고 이 델타값들을 anchor에 적용이 되어서 최종적인 output을 얻는다.

- 델타값 : `x - x'`

## anchor box
- Faster R-CNN에서 언급된 용어로 Bounding box의 후보로 사용되는 상자다. RPN은 sliding window를 사용한다. sliding window는 이미지위에서 직접 지정한 window를 옮겨가는 알고리즘이다. window의 위치를 중심으로 **사용자가 정의한 다양한 크기의 anchor box** 들을 적용해 feature를 추출하는데 image의 크기를 조정할 필요없고,filter 크기를 변경할 필요가 없기 때문에 효율이 좋다.

## IOU
- intersection over union(합집합 over 교집합) : 실제 bounding box와 예측 bounding box가 얼마나 잘 겹쳐지는지

## passthrough
- YOLO는 하나의 feature map에서만 bounding box의 후보를 제안한다. 큰 객체를 검출할 때는 크기가 충분하지만 작은 객체에 대해서는 불충분하다. 그래서 skip-layer를 사용한다., 26x26 크기의 중간 feature map을 skip하여 13x13 layer에 붙인다(concat).

# 참조
- ssd
  + [https://m.blog.naver.com/sogangori/221007697796](https://m.blog.naver.com/sogangori/221007697796)

- yolo
  + [https://m.blog.naver.com/sogangori/220993971883](https://m.blog.naver.com/sogangori/220993971883)
  + [https://m.blog.naver.com/sogangori/221011203855](https://m.blog.naver.com/sogangori/221011203855)

- Faster R-CNN
  + [https://curt-park.github.io/2017-03-17/faster-rcnn/](https://curt-park.github.io/2017-03-17/faster-rcnn/)
